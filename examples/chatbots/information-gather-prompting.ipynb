{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72fae4e-f7de-42b7-91ee-bdd0a57ae46c",
   "metadata": {},
   "source": [
    "# Prompt Generator\n",
    "\n",
    "In this example we will create a chat bot that helps a user generate a prompt.\n",
    "It will first collect requirements from the user, and then will generate the prompt (and refine it based on user input).\n",
    "These are split into two separate states, and the LLM decides when to transition between them.\n",
    "\n",
    "A graphical representation of the system can be found below.\n",
    "\n",
    "![](imgs/prompt-generator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78b593-ba26-4c90-b2e2-83119e47679f",
   "metadata": {},
   "source": [
    "## Gather information\n",
    "\n",
    "First, let's define the part of the graph that will gather user requirements. This will be an LLM call with a specific system message. It will have access to a tool that it can call when it is ready to generate the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53216ab5-2cd3-48a4-8778-41ba10f72519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f795b78-004d-40ca-95d6-069f67e4f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to get information from a user about what type of prompt template they want to create.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "- What the objective of the prompt is\n",
    "- What variables will be passed into the prompt template\n",
    "- Any constraints for what the output should NOT do\n",
    "- Any requirements that the output MUST adhere to\n",
    "\n",
    "If you are not able to discerne this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "\n",
    "After you are able to discerne all the information, call the relevant tool\"\"\"\n",
    "\n",
    "\n",
    "def get_messages_info(messages):\n",
    "    return [SystemMessage(content=template)] + messages\n",
    "\n",
    "\n",
    "class PromptInstructions(BaseModel):\n",
    "    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n",
    "\n",
    "    objective: str\n",
    "    variables: List[str]\n",
    "    constraints: List[str]\n",
    "    requirements: List[str]\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "llm_with_tool = llm.bind_tools([PromptInstructions])\n",
    "\n",
    "chain = get_messages_info | llm_with_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40630f-83c7-4283-a6dd-04231805a7ed",
   "metadata": {},
   "source": [
    "## Generate Prompt\n",
    "\n",
    "We now set up the state that will generate the prompt.\n",
    "This will require a separate system message, as well as a function to filter out all message PRIOR to the tool invocation (as that is when the previous state decided it was time to generate the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca9a0234-bbeb-4bff-8276-8dde499c3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New system prompt\n",
    "prompt_system = \"\"\"Based on the following requirements, write a good prompt template:\n",
    "\n",
    "{reqs}\"\"\"\n",
    "\n",
    "\n",
    "# Function to get the messages for the prompt\n",
    "# Will only get messages AFTER the tool call\n",
    "def get_prompt_messages(messages: list):\n",
    "    tool_call = None\n",
    "    other_msgs = []\n",
    "    for m in messages:\n",
    "        if getattr(m, 'tool_calls', None):\n",
    "            tool_call = m.tool_calls[0][\"args\"]\n",
    "        elif tool_call is not None:\n",
    "            other_msgs.append(m)\n",
    "    return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgs\n",
    "\n",
    "\n",
    "prompt_gen_chain = get_prompt_messages | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbabda8-34f0-4eef-bce2-ad3ff505366b",
   "metadata": {},
   "source": [
    "## Define the state logic\n",
    "\n",
    "This is the logic for what state the chatbot is in.\n",
    "If the last message is a tool call, then we are in the state where the \"prompt creator\" (`prompt`) should respond.\n",
    "Otherwise, if the last message is not a HumanMessage, then we know the human should respond next and so we are in the `END` state.\n",
    "If the last message is a HumanMessage, then if there was a tool call previously we are in the `prompt` state.\n",
    "Otherwise, we are in the \"info gathering\" (`info`) state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f29e15-20e2-420c-a450-84e929f16e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def get_state(messages):\n",
    "    if getattr(messages[-1], 'tool_calls', None):\n",
    "        return \"prompt\"\n",
    "    elif not isinstance(messages[-1], HumanMessage):\n",
    "        return END\n",
    "    for m in messages:\n",
    "        if getattr(m, \"tool_calls\", None):\n",
    "            return \"prompt\"\n",
    "    return \"info\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76bea78-07a5-418f-9b7c-71c376d4b6f7",
   "metadata": {},
   "source": [
    "## Create the graph\n",
    "\n",
    "We can now the create the graph.\n",
    "We will use a SqliteSaver to persist conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d9d6b4-dce4-43cc-9a1a-61a7912ed5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessageGraph, START\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "nodes = {k: k for k in [\"info\", \"prompt\", END]}\n",
    "workflow = MessageGraph()\n",
    "workflow.add_node(\"info\", chain)\n",
    "workflow.add_node(\"prompt\", prompt_gen_chain)\n",
    "workflow.add_conditional_edges(\"info\", get_state, nodes)\n",
    "workflow.add_conditional_edges(\"prompt\", get_state, nodes)\n",
    "workflow.add_edge(START, \"info\")\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf523c-265d-45cf-a981-fc50c50c1738",
   "metadata": {},
   "source": [
    "## Use the graph\n",
    "\n",
    "We can now use the created chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25793988-45a2-4e65-b33c-64e72aadb10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'info':\n",
      "---\n",
      "content='Hello! How can I assist you today?' response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 170, 'total_tokens': 180}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-82cac254-ed31-40c7-8155-9de6596ab47f-0' usage_metadata={'input_tokens': 170, 'output_tokens': 10, 'total_tokens': 180}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'info':\n",
      "---\n",
      "content='Sure! I can help you with that. To create an extraction prompt, I need some information from you. \\n\\nCan you please provide the following details:\\n1. What is the objective of the prompt?\\n2. What variables will be passed into the prompt template?\\n3. Any constraints for what the output should NOT do?\\n4. Any requirements that the output MUST adhere to?\\n\\nOnce I have this information, I can assist you in creating the extraction prompt template.' response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 193, 'total_tokens': 287}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-b66dbf2e-de98-4022-891e-2f6c8e8676f6-0' usage_metadata={'input_tokens': 193, 'output_tokens': 94, 'total_tokens': 287}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'info':\n",
      "---\n",
      "content='Great! Thank you for providing the objective of the prompt. \\n\\nNow, could you please specify the variables that will be passed into the prompt template for populating the CSAT record in your CRM? \\n\\nAdditionally, are there any constraints for what the output should NOT do and any requirements that the output MUST adhere to? \\n\\nOnce I have this information, I can help you create the extraction prompt template for populating the CSAT record in your CRM.' response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 306, 'total_tokens': 398}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-26e74d73-5d7c-4731-b2fc-f345cdf81bba-0' usage_metadata={'input_tokens': 306, 'output_tokens': 92, 'total_tokens': 398}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'info':\n",
      "---\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_hjFAEvsaJhpeJ5001ZdCEyst', 'function': {'arguments': '{\"objective\":\"Create an extraction prompt template to populate a CSAT record in a CRM\",\"variables\":[\"comment\",\"score\"],\"constraints\":[\"Output should be valid JSON\",\"Output should reflect/summarize the user\\'s inputs\"],\"requirements\":[\"Output must adhere to the specified variables\"]}', 'name': 'PromptInstructions'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 432, 'total_tokens': 496}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None} id='run-38042cdf-8574-4565-bb39-419ee777819d-0' tool_calls=[{'name': 'PromptInstructions', 'args': {'objective': 'Create an extraction prompt template to populate a CSAT record in a CRM', 'variables': ['comment', 'score'], 'constraints': ['Output should be valid JSON', \"Output should reflect/summarize the user's inputs\"], 'requirements': ['Output must adhere to the specified variables']}, 'id': 'call_hjFAEvsaJhpeJ5001ZdCEyst'}] usage_metadata={'input_tokens': 432, 'output_tokens': 64, 'total_tokens': 496}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'prompt':\n",
      "---\n",
      "content='## CSAT Record Extraction Prompt Template\\n\\n### Objective:\\nCreate an extraction prompt template to populate a CSAT record in a CRM.\\n\\n### Variables:\\n1. `comment`: (string) The feedback comment provided by the customer.\\n2. `score`: (integer) The satisfaction score given by the customer.\\n\\n### Constraints:\\n- Output should be valid JSON.\\n- Output should reflect/summarize the user\\'s inputs.\\n\\n### Requirements:\\nOutput must adhere to the specified variables.\\n\\n#### Prompt Template:\\n```json\\n{\\n  \"comment\": \"Please provide your feedback here.\",\\n  \"score\": \"On a scale of 1-10, how satisfied are you with our service?\"\\n}\\n```' response_metadata={'token_usage': {'completion_tokens': 138, 'prompt_tokens': 82, 'total_tokens': 220}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-01ae6136-5c1e-41f8-9746-93304dfd489d-0' usage_metadata={'input_tokens': 82, 'output_tokens': 138, 'total_tokens': 220}\n",
      "\n",
      "---\n",
      "\n",
      "AI: Byebye\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "while True:\n",
    "    user = input(\"User (q/Q to quit): \")\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: Byebye\")\n",
    "        break\n",
    "    for output in graph.stream([HumanMessage(content=user)], config=config):\n",
    "        # stream() yields dictionaries with output keyed by node name\n",
    "        for key, value in output.items():\n",
    "            print(f\"Output from node '{key}':\")\n",
    "            print(\"---\")\n",
    "            print(value)\n",
    "        print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a276d20e-8a1b-4add-bf8d-83a8c803431d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
