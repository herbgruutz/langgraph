{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/herbgruutz/langgraph/blob/notebooks/Introduction_to_langgraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a1aae78-88a6-4133-b905-7e46c8e3772f",
      "metadata": {
        "id": "4a1aae78-88a6-4133-b905-7e46c8e3772f"
      },
      "source": [
        "# Introduction to LangGraph\n",
        "\n",
        "In this tutorial, we will build a support chatbot in LangGraph that can:\n",
        "\n",
        "- Answer common questions by searching the web\n",
        "- Maintain conversation state across calls\n",
        "- Route complex queries to a human for review\n",
        "- Use custom state to control its behavior\n",
        "- Rewind and explore alternative conversation paths\n",
        "\n",
        "We'll start with a basic chatbot and progressively add more sophisticated capabilities, introducing key LangGraph concepts along the way.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Setup\n",
        "\n",
        "First, install the required packages:"
      ],
      "metadata": {
        "id": "G6NJvF0I0XJJ"
      },
      "id": "G6NJvF0I0XJJ"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6f11d631-8679-4f28-822f-cdf1f2ddc21c",
      "metadata": {
        "id": "6f11d631-8679-4f28-822f-cdf1f2ddc21c"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langsmith\n",
        "%pip install -U langchain-groq\n",
        "%pip install -U tavily-python\n",
        "%pip install langchain-community langchainhub"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6d1e870-1bc0-4d44-86c0-96681ccf6113",
      "metadata": {
        "id": "a6d1e870-1bc0-4d44-86c0-96681ccf6113"
      },
      "source": [
        "Next, set your API keys:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set Environment\n",
        "project_name = \"Introduction To Langgraph\" # @param {type:\"string\"}\n",
        "api_keys = \"anthropic_api_key\"\n",
        "import getpass\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "def is_colab():\n",
        "    \"\"\"Detect if the code is running in Google Colab\"\"\"\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "def set_env(var: str, api_keys: list):\n",
        "    \"\"\"Set environment variables for API keys\"\"\"\n",
        "    for key in api_keys:\n",
        "        if not os.environ.get(key.upper()):\n",
        "            if is_colab():\n",
        "                os.environ[key.upper()] = userdata.get(key)\n",
        "            else:\n",
        "                os.environ[key.upper()] = getpass.getpass(f\"{key.upper()}: \")\n",
        "\n",
        "api_keys = [\n",
        "    \"tavily_api_key\",\n",
        "    \"langchain_api_key\",\n",
        "    \"groq_api_key\",\n",
        "    # \"anthropic_api_key\"\n",
        "]\n",
        "\n",
        "for key in api_keys:\n",
        "    set_env(key, api_keys)\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = project_name"
      ],
      "metadata": {
        "id": "4Eft_6eAS02_"
      },
      "id": "4Eft_6eAS02_",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ef7bcad1-1274-4b7c-a2e9-365180ef3a31",
      "metadata": {
        "id": "ef7bcad1-1274-4b7c-a2e9-365180ef3a31"
      },
      "source": [
        "## Part 1: Build a Basic Chatbot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Define `StateGraph`\n",
        "\n",
        "A `StateGraph` object defines the structure of our chatbot as a \"state machine\". We'll add `nodes` to represent the llm and functions our chatbot can call and `edges` to specify how the bot should transition between these functions."
      ],
      "metadata": {
        "id": "s_JxN8jaCh7A"
      },
      "id": "s_JxN8jaCh7A"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e58df974-7579-4f25-9d91-66389b94eba2",
      "metadata": {
        "id": "e58df974-7579-4f25-9d91-66389b94eba2"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    # Messages have the type \"list\". The `add_messages` function\n",
        "    # in the annotation defines how this state key should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4137feed-746e-4c72-a34a-f7a699ad5dcf",
      "metadata": {
        "id": "4137feed-746e-4c72-a34a-f7a699ad5dcf"
      },
      "source": [
        "**Notice** that we've defined our `State` as a TypedDict with a single key: `messages`. The `messages` key is annotated with the [`add_messages`](https://langchain-ai.github.io/langgraph/reference/graphs/?h=add+messages#add_messages) function, which tells LangGraph to append new messages to the existing list, rather than overwriting it.\n",
        "\n",
        "So now our graph knows two things:\n",
        "\n",
        "1. Every `node` we define will receive the current `State` as input and return a value that updates that state.\n",
        "2. `messages` will be _appended_ to the current list, rather than directly overwritten. This is communicated via the prebuilt [`add_messages`](https://langchain-ai.github.io/langgraph/reference/graphs/?h=add+messages#add_messages) function in the `Annotated` syntax.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define `chatbot` Node\n",
        "\n",
        "Nodes represent units of work. They are typically regular python functions."
      ],
      "metadata": {
        "id": "A6dwHE8ICnvu"
      },
      "id": "A6dwHE8ICnvu"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bc8c9137-8261-42ea-8e83-3590981d23e2",
      "metadata": {
        "id": "bc8c9137-8261-42ea-8e83-3590981d23e2"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "# The first argument is the unique node name\n",
        "# The second argument is the function or object that will be called whenever\n",
        "# the node is used.\n",
        "graph_builder.add_node(\"chatbot\", chatbot)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c1dcd9-fb86-4649-81b4-ff6ce20a2e46",
      "metadata": {
        "id": "b6c1dcd9-fb86-4649-81b4-ff6ce20a2e46"
      },
      "source": [
        "**Notice** how the `chatbot` node function takes the current `State` as input and returns an updated `messages` list. This is the basic pattern for all LangGraph node functions.\n",
        "\n",
        "The `add_messages` function in our `State` will append the llm's response messages to whatever messages are already in the state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finish Graph\n",
        "\n",
        "Next, add an `entry` point. This tells our graph **where to start its work** each time we run it."
      ],
      "metadata": {
        "id": "CNKZle3vC73t"
      },
      "id": "CNKZle3vC73t"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e331e10d-ebcf-4144-9bd3-999b4d656dd3",
      "metadata": {
        "id": "e331e10d-ebcf-4144-9bd3-999b4d656dd3"
      },
      "outputs": [],
      "source": [
        "graph_builder.set_entry_point(\"chatbot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0499c318-d1e6-46fa-a652-8f9e65313355",
      "metadata": {
        "id": "0499c318-d1e6-46fa-a652-8f9e65313355"
      },
      "source": [
        "Similarly, set a `finish` point. This instructs the graph **\"any time this node is run, you can exit.\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "075f0929-3591-4852-b2d3-eaadde40662d",
      "metadata": {
        "id": "075f0929-3591-4852-b2d3-eaadde40662d"
      },
      "outputs": [],
      "source": [
        "graph_builder.set_finish_point(\"chatbot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65a9b88c-2c53-4d95-8eb1-d544a8946f65",
      "metadata": {
        "id": "65a9b88c-2c53-4d95-8eb1-d544a8946f65"
      },
      "source": [
        "Finally, we'll want to be able to run our graph. To do so, call \"`compile()`\" on the graph builder. This creates a \"`CompiledGraph`\" we can use invoke on our state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0bb67a01-cf5c-4625-8c07-6e8c0af50fca",
      "metadata": {
        "id": "0bb67a01-cf5c-4625-8c07-6e8c0af50fca"
      },
      "outputs": [],
      "source": [
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c39407b-d6f6-48a4-b1f6-31fc7f88b275",
      "metadata": {
        "id": "0c39407b-d6f6-48a4-b1f6-31fc7f88b275"
      },
      "source": [
        "### Visualize Graph\n",
        "\n",
        "You can visualize the graph using the `get_graph` method and one of the \"draw\" methods, like `draw_ascii` or `draw_png`. The `draw` methods each require additional dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "32e4f36e-72ce-4ade-bd7e-94880e0d456b",
      "metadata": {
        "id": "32e4f36e-72ce-4ade-bd7e-94880e0d456b",
        "outputId": "3e0e7990-f542-4f64-bfbe-1b62a6f3c97e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCADaAGIDASIAAhEBAxEB/8QAHQABAAMBAQADAQAAAAAAAAAAAAUGBwgEAQMJAv/EAE4QAAEDAwEDBgkGBxADAQAAAAECAwQABQYRBxIhExYxVZTRCBciQVF0k7ThFBU4YXWyNTZSVmJxgQkYIyQyMzdCRlSCkZKxs9JyhJWh/8QAGgEBAAIDAQAAAAAAAAAAAAAAAAQFAQIDBv/EADcRAAIAAwQGBgkFAQAAAAAAAAABAgMRBBMhkRIVMUFRUgUUYXGhsSIyNGJygcHR8DNCU2Phwv/aAAwDAQACEQMRAD8A/VFa0tpKlEJSkakk6ACo3nVZeuIHaUd9Mq/Fi8epvfcNZZYLBbF2K3KVboilGM2SSwnU+SPqrjPny7NLUcabq6YE2z2e/rjShqfOqy9cQO0o76c6rL1xA7SjvrO+b1r6th+wR3U5vWvq2H7BHdVfrWz8kWaJmrve8DROdVl64gdpR3051WXriB2lHfWd83rX1bD9gjupzetfVsP2CO6mtbPyRZoau97wNE51WXriB2lHfTnVZeuIHaUd9Z3zetfVsP2CO6nN619Ww/YI7qa1s/JFmhq73vA0TnVZeuIHaUd9OdVl64gdpR31nfN619Ww/YI7qc3rX1bD9gjuprWz8kWaGrve8DROdVl64gdpR317Yc+NcWi7EkNSmgd0rZWFjX0aisu5vWvq2H7BHdU3skjtRWsoaZbQy0m7nRDaQlI/isfoAqbZrXKtekoE00q404pfUjWiyXEGlWpfaUpUkryLyr8WLx6m99w1nePfgC2+rNfcFaJlX4sXj1N77hrO8e/AFt9Wa+4KqelfZ4PifkXPR37iQpSleVLopETbRh9wyOfYol1XKucEvIfbYhPuIC2klTraXAgoWtIB1QlRVrw01qvbNPCJsGdbPZeUzm5VlZhb6paHoUnk208sttvccU0kPEhA1DepBOhANVfFfnWw7b/kWJWfJ7fjdwuE5/IYd5gFFtQvdUUy4j587roSdxKiCFklKCKgccuGZ4rsJnYharDkVtyazTXEypTFuKuUiLuClOuQnFAoec5BwqSBqdQeGoFTrqClF2b++u4h3kVavt3dxsdt244TdsWvmRRr1ra7IkruSnIj7b0Ubu9qtlSA4NRxHk8fNrVazPwmcax22WafbkTbxFn3iPbFSGrdL5MIcOqnWlBkh7RPFIRrvE8CdNKxy54pdJNo22JtWP5nIiX3Foqbc7fmZL8qa60X0rSOU3lpVq4ndbUEq01ITu1s222yz04Nh8u2WmVcU4/fbZcpEC3slx/5OysBYbbHFSkg67o48KzdSoYktte3sX1F5Mihb4fc1K1XNi9WyLPi8r8mlNJeb5ZlbK91Q1G8hYCknQ9CgCPOK9dR9hvKMgtEa4txZkJEhO8GJ8dTD6OJHltqAKTw6DUhUF4MlrFCvdsr/tV9sH3WPXhr3bK/7VfbB91j1f8AQ/rzfh/6hK63/pLvL1SlK9AeeIvKvxYvHqb33DWc2NpD+OW9txIW2uI2lSVDUEFA1BrU5sRufDfiuglp9tTawDodCND/AL1TWdklujsoabu16Q2hISlIm8ABwA6KjWqzK1SlBpUadSwstohkV0t5mI8H/ZmCCMAxsEecWtn/AK0/e/bMvzAxv/5bP/WtR8VUHri99t+FPFVB64vfbfhVdqyZ/N5kzrkjl8ERLDDcZhtllCWmm0hCEIGgSkDQAD0V9lSXiqg9cXvtvwp4qoPXF77b8K56n/tWTOmsJXBkbSs08FOLN2u7FLVk2Q3u6OXSRKmNOKjyOTRutyXG0aJA/JSK13xVQeuL3234U1P/AGrJjWErgzPb7sdwXKLq/c7xh9kulxf3eVly4DTjrmiQkbyikk6AAfqArwq2BbNFhIVgWOKCRokG2M8BrroPJ9JP+dah4qoPXF77b8KeKqD1xe+2/Cui6LjWCneZp12Q/wBvgis45i9nxC2Jt1jtcS0QEqKxFhMpabCj0ndSANTVi2V/2q+2D7rHr7PFVB64vfbfhU7i+KxMSiyWIjsh75S+ZDrkpzlFqWUpT0/qQkfsqbY7H1RxxOPScSpv4p/QjWm1QTpehCiZpSlTSrFKUoBSlKAUpSgOd/AE+jJYfXrl769XRFc7+AJ9GSw+vXL316uiKAUpSgFKUoBSlKAUpSgFKUoBSlKA538AT6Mlh9euXvr1dEVzv4An0ZLD69cvfXq6IoBSlKAUpSgFKUoBSlfBISCSQAOJJoD5pVGnbTBIcKLBbzdmwQPlzr3IxVfWhWilOD60p3Tw0V6I85plquIiWVH6JceVp+3Qf7V3uWvWaXeyTDZpsSqoTSa4i/dO9hasuwG37RbZHC7njo+TT9weU5CWvyT6TybitdPQ6snorpPnnl392sn+p6vDe7zkOSWafablbrDMt05hcaTHcLxS62tJSpJ+ogkUulzLM26pO4H5ufueGxFzaltziX+W0v5jxJTdzdcHAKlBWsZvX076Sv8AU0R56/XuuZ/B82Zz/BzwZzG7Ai2TEvy3JkibLLnKvLVoBrugABKEpSAOHAnpJrTueeXf3ayf6nqXS5lmOqTuBpVKzUZnluo1jWXTz6Ker0Rtot5hqBudhafj/wBZ21SeUcT9fJuJTqP/ABUT6AfOuq7Ik/n9zDss5Y6JoVK8NmvcLIICJlvkJkR1EjUApUlQ6UqSQClQ86SAR5xXuri04XRkXYKUpWAKzzOLsq93hdgbURb4yEuT90/z6lA7jCv0d3y1D+tqgcUlQOh1kUBanrxkjrn86q6vBXp0SEoTr/gSn9mldoPRhijW1bPmTbJAo5mO49/RXlut2g2K3SLhcpke3wI6d96VKdS000n0qUogAfWaqe2rO5WzTZjfMigx2pM+MhtqM2+SG+WddQy2V6cd0KcBOnmB6KzHbdjGV2HwftobuRZo5lCXbMocgu2sRUsuajUoLYB3fNuq3j+lUQvI49GtFsR0ICCNRxFKxOLlWTbPtoC7NlOXsXWzzcbl3n5c7b244trkdbYWUhH8prdd10WVKG5/KOtVzZptFzO6Z9Bx+4Xq9zLRkdlly7fdrrZYkB5p1st7r0dCCrVBS6Duvo11CekEihreqtKHQdxvVus8Bc6fPiwoSFJQqTIeS22lSlhCQVEgalRCQPOSB017K44tVnucTwG2HncgkzkzFW0xWJEdkNwSLm2DubiEqWCSCd9Sjw4EVf8AMNqOYbC7vf4d7u6M2jKxuXfLc89DbjPMPsLQgtOBoBKmjyqTvaBQ0I49NZoaqdhVrCiOiK/lDiXU7yFBadSNUnUag6GsPjXTO8SzDDrHfMy+e0ZjFmRy81bo7K7XLbjF5LjG6khaNAsbroXxCSSQSK+7wP7TPgbDsdky75KuceVGCo8R9llCIYDjgIQpCEqVvagnfKjw4aVg3UysWjT8w+5sKrmrEp3z20SmMnQXBre0QtngC4R+U2PK16SkFPo01gEEajiKy2Yy3IiPtO6FpaFJXr0aEaGrls7kvTNn+MyJBJfdtkVbhPSVFpJP/wC1K9eVpPanT5PZlRlXboEolEt5YaUpXIqxWZZFAVYcwkrUCIV5KXmVk+SJCWwlbf1EobCx6dHD5iTpteO72iJfbe7CmtB6O5pqNSCkg6hSVDilQIBChoQQCDqK6QRJVUWxneTNcqNRGUZVi9szXHLhYrzFTNtc9ksSGFEjeSfQRxBHAgjiCARVCd2Aw52K3vHrpmGWXq33SCbeoXGe24phrUHVv+CAK+AG+sKOnnrVJ2M5DY3N1pgZDCBAS60tDUpI/TSrdQo/Wkp1/JHnjjOuKeCsbvSVecCMlWn7QoilxG/Vo+5/jLxTZMzGpXcp2VWPMr4i5XT5Q/paJdkXFCwGXY8nc5Xe4b29/BgAhQ01PDo0hcZ2EW7G8lsd9XkeR3e42Zh2JFVc5jbiBHWgJLJSltI0GiVbwAWShO8ogaVfPnCf+bl67J8afOE/83L12T406vN4G2nJbrVGcN+DlYWsQuuKi95AcbmvNvNW0zEcnB3JIkBLCuT30grTpxUogdBB0IkbLsMsUJ+8ybxOuuXTLrBVa35N/kpeWmGrUqYQEJQlKCTqdBqSASeFTmIbQoefWJm9Y9brpdrW8tbbcqPF1QpSFFCwNT5lJI/ZU184T/zcvXZPjTq83gY0pPFFJw7YfacRv8C8O3m+5DLtkZcS2C9y0vJt7SgAsNBKE8SEpSVL3laDTWpPZzsug7MGZcS1XW6v2p1ZVHtk19DkeCCtSylnRAUEkrPBSldA0qxifPJA5uXof+p8a9EaHkd1UERLA7BSemTdXkNoT/gQpa1H6tEg+kcdHV5m9U72heSYcao815ZfuTKbTDUROuOsdtSTxaQeC3f1ISSf17o84rWIkVqDFZjMJCGWUJbQkeZIGgH+QqGxfEWsdS4+898vujw0emqQEEj8hCeO4gHoTqfSSTxqfrMTShUEO7x/wprTPvosNiFKUrkRBSlKAUpSgFKUoDnfwBPoyWH165e+vV0RXO/gCfRksPr1y99eroigFKUoBSlKAUpSgFKUoBSlKAUpSgOd/AE+jJYfXrl769XRFc7+AJ9GSw+vXL316uiKAUpSgFKUoBSlKAUpXjm3m321xLcudGirUN4JeeSgkenQmspOJ0QPZSovnVZeuIHaUd9OdVl64gdpR31vdx8rM0ZKVzf4WXhc3LwXbhYd7A+ctnuzS9y4Ju3yXk30HymlI5Bf9VSFA7w11UNPJJrfOdVl64gdpR31kvhS7Pse297F75jIudsN1Sj5ZanVyWxyctsEo468AoFTZPmDhpdx8rFGcl+A/wCGRPg8ztj9uwBV1el3J7lLqi7bnIsuvreddLXInUNoUo6b43tzza1+kVcAfuZ+x+Dhlsve0LJXGLfeZqlWy3RpriW3GmEqHLObqjqCtaQkagEBtXmVXdfOqy9cQO0o76XcfKxRkpSovnVZeuIHaUd9OdVl64gdpR30u4+VijJSlRreS2h5xDbd1hLcWQlKUyEEknoAGtSVauFw7UYFKUrUCsszGBFn7THxJjMyAm0R93lWwrT+Gf6Na1Os0yb+kyT9kRv+aRSNuGTMa4fVFd0i2rJG12eaPFzetfVsP2CO6nN619Ww/YI7qkKV5y9mczzPB6cXEj+b1r6th+wR3U5vWvq2H7BHdXnyvL7Pg9mcut8nt2+ChSUcosFRUtR0ShCUgqWonoSkEnzCqyzt3wR3HJl9OQNsW2FJZiS1yWHWXIzrqkpbDra0BbYUVDylJA01OugJrZTJrxTfidFexKqr4lu5vWvq2H7BHdTm9a+rYfsEd1QON7WMUyuPd3oF1CE2hAcnpnMOw1xmykqDi0vJQoIKUqIXpukA6HhVNsPhC2nONq2O45i8pq42qbbZsyU+7DkMuAtqZDRaLgSFNq33PKAUDujQjQ65UU7HF4d5soJzrtw27e81Dm9a+rYfsEd1Ob1r6th+wR3VIUrS9mczzOOnFxK5kNmt8WLDdZgxmnU3GDotDKUkfxproIFbXWQZR+D4v2jB97arX6vrNFFFZU4nX0ovKE9j0O27M68z8kKUpXUvBWaZN/SZJ+yI3/NIrS6zTJv6TJP2RG/5pFazP0Jvd9UVvSPskz5eaP7pVaynZpiWcSmZOQ41ar3IZRybbtwhtvKQnXXdBUDoNTrUL+9/2Z6Acwcc0HHT5sZ0+7Xm1o72eFSgpi3l/pWPCSxO5XyPht3hwrtdYFivHyu4QLFJcYmrZUy40XGVNqSsrQVg7qSCQVCqRf8ACLfdsFuV1xrGszTc5l9srUhWSGY/LksR5jTm+lD61uJbQFuakhOmij0ca37FMAxnBRKGOWC22ISt0vi3xUM8ru67u9uga6bytNfSan66KZopJbiRDaHAlDDsXy31xXec3bc9nmRZll20VizW2Q8J+G29plZQUMy3mpz7q44cI3d9Tfk6a8A4NdAambLkMrPtuGDXaLiOSWK226yXJiQu72pyK2y4tUbda1I018hWhHA6eSTodN4rwXyxW7JrVItl2gx7nbpAAeiy2g424AQRvJPA8QD+yl5hRr82BWj0VC1+NUZ76VQUbAdmjZ1TgOOJOhGotjI4EaEfyfRX3W/Ybs7tM+NOhYPj8SZGdS8w+zbmkrbWkgpUkhOoIIBBHorn6PE4Ul8Xl/pYco/B8X7Rg+9tVr9ZBlH4Pi/aMH3tqtfq/snsq+KLyhPXdDezP4n5IUpSu5eiqzkOAQMiuwuTsqdElBhMcqhv8mFISpSgCND51q/zqzUraGJw7DDSiVGqopPiqg9cXvtvwp4qoPXF77b8Ku1K2vH2ZI5XMrkWSKT4qoPXF77b8KeKqD1xe+2/CrtSl4+zJC5lciyRSfFVB64vfbfhTxVQeuL3234VdqUvH2ZIXMrkWSKT4qoPXF77b8KeKqD1xe+2/CrtSl4+zJC5lciyRSRsnthdYW7cbtISy82+G3peqCpCwtOo04jVIq7UpWIo3EqM6QwwwKkKoKUpWhsf/9k=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a98097a3-a126-4081-b21e-697ec1185fff",
      "metadata": {
        "id": "a98097a3-a126-4081-b21e-697ec1185fff"
      },
      "source": [
        "### Run Graph\n",
        "\n",
        "Now let's run the chatbot!\n",
        "\n",
        "**Tip:** You can exit the chat loop at any time by typing \"quit\", \"exit\", or \"q\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7afb4c9a-7404-4e92-9945-36f372015f08",
      "metadata": {
        "id": "7afb4c9a-7404-4e92-9945-36f372015f08",
        "outputId": "5c294d43-02f0-4d68-ca2b-3d20410faa59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Wrte jupyter code snippet that when run in a notebook will do something interesting with javascript\n",
            "Assistant: Sure! Here is a code snippet that uses the `IPython.display` module to display a JavaScript alert message in a Jupyter notebook:\n",
            "\n",
            "```\n",
            "from IPython.display import Javascript\n",
            "\n",
            "Javascript(\"alert('Hello, Jupyter!')\")\n",
            "```\n",
            "\n",
            "When you run this code in a Jupyter notebook, it will display an alert message with the text \"Hello, Jupyter!\".\n",
            "\n",
            "Here is another example that uses the `display` function from the `IPython.display` module to display a JavaScript countdown timer:\n",
            "\n",
            "```\n",
            "from IPython.display import display, Javascript\n",
            "\n",
            "def countdown(t):\n",
            "    \"\"\"Display a countdown timer for the given number of seconds.\"\"\"\n",
            "    display(Javascript(\"\"\"\n",
            "        function countdown(t) {\n",
            "            var seconds = t;\n",
            "            var timer = setInterval(function() {\n",
            "                minutes = parseInt(seconds/60, 10)\n",
            "                seconds = seconds % 60;\n",
            "                minutes = minutes < 10 ? \"0\" + minutes : minutes;\n",
            "                seconds = seconds < 10 ? \"0\" + seconds : seconds;\n",
            "                document.body.innerHTML = minutes + \":\" + seconds;\n",
            "                if (--seconds < 0) {\n",
            "                    clearInterval(timer);\n",
            "                    document.body.innerHTML = \"Times up!\";\n",
            "                }\n",
            "            }, 1000);\n",
            "        }\n",
            "    \"\"\"))\n",
            "    countdown(t)\n",
            "\n",
            "countdown(10)  # count down from 10 seconds\n",
            "```\n",
            "\n",
            "This code defines a `countdown` function that takes a number of seconds as an argument and displays a countdown timer for that number of seconds.\n",
            "\n",
            "I hope these examples give you some ideas for how you can use JavaScript in Jupyter notebooks! Let me know if you have any questions.\n",
            "User: Give an interactive example\n",
            "Assistant: Sure, here's an example of an interactive program that asks the user to guess a number between 1 and 10:\n",
            "\n",
            "```python\n",
            "import random\n",
            "\n",
            "def guess_the_number():\n",
            "    number_to_guess = random.randint(1, 10)\n",
            "    guess = None\n",
            "\n",
            "    while guess != number_to_guess:\n",
            "        guess = int(input(\"Guess a number between 1 and 10: \"))\n",
            "        if guess < number_to_guess:\n",
            "            print(\"Too low!\")\n",
            "        elif guess > number_to_guess:\n",
            "            print(\"Too high!\")\n",
            "        \n",
            "    print(\"Congratulations! You guessed the number.\")\n",
            "\n",
            "guess_the_number()\n",
            "```\n",
            "\n",
            "When you run this program, it will generate a random number between 1 and 10 and ask you to guess the number. If your guess is too low or too high, it will give you a hint and ask you to guess again. The program will keep asking you to guess until you get the number right. Once you guess the number, it will congratulate you and end the program.\n",
            "User: bye\n",
            "Assistant: Goodbye! I'm glad I could assist you today. If you have any more questions or need further assistance in the future, don't hesitate to ask. Have a great day!\n",
            "User: q\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    for event in graph.stream({\"messages\": (\"user\", user_input)}):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98e1cdb5-869a-41ea-9dab-e28cfc524499",
      "metadata": {
        "id": "98e1cdb5-869a-41ea-9dab-e28cfc524499"
      },
      "source": [
        "### Snippet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e014741f-cfad-4045-b6fa-ebd74ae93a5a",
      "metadata": {
        "id": "e014741f-cfad-4045-b6fa-ebd74ae93a5a"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "llm = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
        "\n",
        "\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "# The first argument is the unique node name\n",
        "# The second argument is the function or object that will be called whenever\n",
        "# the node is used.\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "graph_builder.set_finish_point(\"chatbot\")\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f22c5d4a-3134-413c-81fe-dd9752fbeb66",
      "metadata": {
        "id": "f22c5d4a-3134-413c-81fe-dd9752fbeb66"
      },
      "source": [
        "## Part 2: Add Tool to Chatbot\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import create_openai_tools_agent\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults, TavilyAnswer\n",
        "from langchain.agents import AgentExecutor\n"
      ],
      "metadata": {
        "id": "Gg9EPOx9YUgK"
      },
      "id": "Gg9EPOx9YUgK",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "591ce9ba-c431-4165-b815-25c944ef7cdb",
      "metadata": {
        "id": "591ce9ba-c431-4165-b815-25c944ef7cdb"
      },
      "source": [
        "### Define Tool"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code raise an error.\n",
        "chat = ChatGroq(\n",
        "    temperature=0,\n",
        "    groq_api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model_name=\"mixtral-8x7b-32768\",\n",
        ")\n",
        "\n",
        "\n",
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "agent = create_openai_tools_agent(chat, tools, prompt)\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, stream_runnable = False)\n",
        "agent_executor.invoke({\"input\": \"What is Langchain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYVrygWaYgVO",
        "outputId": "1538a1e5-c998-45eb-db85-1dc34d52249c",
        "collapsed": true
      },
      "id": "WYVrygWaYgVO",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `tavily_search_results_json` with `{'query': 'Langchain'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://github.com/langchain-ai/langchain', 'content': 'About\\n⚡ Building applications with LLMs through composability ⚡\\nResources\\nLicense\\nCode of conduct\\nSecurity policy\\nStars\\nWatchers\\nForks\\nReleases\\n291\\nPackages\\n0\\nUsed by 39k\\nContributors\\n1,848\\nLanguages\\nFooter\\nFooter navigation Latest commit\\nGit stats\\nFiles\\nREADME.md\\n🦜️🔗 LangChain\\n⚡ Building applications with LLMs through composability ⚡\\nLooking for the JS/TS library? ⚡ Building applications with LLMs through composability ⚡\\nLicense\\nlangchain-ai/langchain\\nName already in use\\nUse Git or checkout with SVN using the web URL.\\n 📖 Documentation\\nPlease see here for full documentation, which includes:\\n💁 Contributing\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n What can you build with LangChain?\\n❓ Retrieval augmented generation\\n💬 Analyzing structured data\\n🤖 Chatbots\\nAnd much more!'}, {'url': 'https://en.wikipedia.org/wiki/LangChain', 'content': 'In October 2023 LangChain introduced LangServe, a deployment tool designed to facilitate the transition from LCEL (LangChain Expression Language) prototypes to production-ready applications.[5]\\nIntegrations[edit]\\nAs of March 2023, LangChain included integrations with systems including Amazon, Google, and Microsoft Azure cloud storage; API wrappers for news, movie information, and weather; Bash for summarization, syntax and semantics checking, and execution of shell scripts; multiple web scraping subsystems and templates; few-shot learning prompt generation support; finding and summarizing \"todo\" tasks in code; Google Drive documents, spreadsheets, and presentations summarization, extraction, and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic, and Hugging Face language models; iFixit repair guides and wikis search and summarization; MapReduce for question answering, combining documents, and question generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF file text extraction and manipulation; Python and JavaScript code generation, analysis, and debugging; Milvus vector database[6] to store and retrieve vector embeddings; Weaviate vector database[7] to cache embedding and data objects; Redis cache database storage; Python RequestsWrapper and other methods for API requests; SQL and NoSQL databases including JSON support; Streamlit, including for logging; text mapping for k-nearest neighbors search; time zone conversion and calendar operations; tracing and recording stack symbols in threaded and asynchronous subprocess runs; and the Wolfram Alpha website and SDK.[8] As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.[2]\\nHistory[edit]\\nLangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. In April 2023, LangChain had incorporated and the new startup raised over $20 million in funding at a valuation of at least $200 million from venture firm Sequoia Capital, a week after announcing a $10 million seed investment from Benchmark.[3][4]\\n The project quickly garnered popularity, with improvements from hundreds of contributors on GitHub, trending discussions on Twitter, lively activity on the project\\'s Discord server, many YouTube tutorials, and meetups in San Francisco and London. As of April 2023, it can read from more than 50 document types and data sources.[9]\\nReferences[edit]\\nExternal links[edit]'}]\u001b[0m\u001b[32;1m\u001b[1;3mBased on the information provided by the tool, Langchain is an open-source project launched in October 2022 by Harrison Chase while working at a machine learning startup called Robust Intelligence. It was later incorporated in April 2023 and raised over $20 million in funding from Sequoia Capital and Benchmark. Langchain can read from more than 50 document types and data sources, and it has integrations with various systems, including Amazon, Google, and Microsoft Azure cloud storage, API wrappers for news, movie information, and weather, and Bash for summarization, syntax and semantics checking, and execution of shell scripts, among others. It is a language model integration framework, and its use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is Langchain?',\n",
              " 'output': 'Based on the information provided by the tool, Langchain is an open-source project launched in October 2022 by Harrison Chase while working at a machine learning startup called Robust Intelligence. It was later incorporated in April 2023 and raised over $20 million in funding from Sequoia Capital and Benchmark. Langchain can read from more than 50 document types and data sources, and it has integrations with various systems, including Amazon, Google, and Microsoft Azure cloud storage, API wrappers for news, movie information, and weather, and Bash for summarization, syntax and semantics checking, and execution of shell scripts, among others. It is a language model integration framework, and its use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIezVSiLrGnL",
        "outputId": "f3dca71d-1f67-43c0-eb39-63abebbf1066"
      },
      "id": "eIezVSiLrGnL",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-tools-agent', 'lc_hub_commit_hash': 'c18672812789a3b9697656dd539edf0120285dcae36396d0b548ae42a4ed66f5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f503f02-d23d-42e8-9b5d-eb2681b242f4",
      "metadata": {
        "id": "7f503f02-d23d-42e8-9b5d-eb2681b242f4"
      },
      "source": [
        "### Define Graph\n",
        "\n",
        "We have added `bind_tools` on our LLM. This lets the LLM know the correct JSON format to use if it wants to use our search engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "dc5af88b-47d2-43bf-9a2c-6c07506b1732",
      "metadata": {
        "id": "dc5af88b-47d2-43bf-9a2c-6c07506b1732"
      },
      "outputs": [],
      "source": [
        "graph_builder = StateGraph(State)\n",
        "\n",
        "\n",
        "# Modification: tell the LLM which tools it can call\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1e84cfc-b1b2-48e3-8550-152a408c3926",
      "metadata": {
        "id": "d1e84cfc-b1b2-48e3-8550-152a408c3926"
      },
      "source": [
        "### Define `BasicToolNode`\n",
        "\n",
        "Next we need to create a function to actually run the tools if they are called. We'll do this by adding the tools to a new node.\n",
        "\n",
        "Below, implement a `BasicToolNode` that checks the most recent message in the state and calls tools if the message contains `tool_calls. It relies on the LLM's `tool_calling` support, which is available in Anthropic, OpenAI, Google Gemini, and a number of other LLM providers.\n",
        "\n",
        "We will later replace this with LangGraph's prebuilt [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode) to speed things up, but building it ourselves first is instructive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "12f1fc14-cd91-4cd4-9f2e-1d007f8beafc",
      "metadata": {
        "id": "12f1fc14-cd91-4cd4-9f2e-1d007f8beafc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "\n",
        "class BasicToolNode:\n",
        "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
        "\n",
        "    def __init__(self, tools: list) -> None:\n",
        "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
        "\n",
        "    def __call__(self, inputs: dict):\n",
        "        if messages := inputs.get(\"messages\", []):\n",
        "            message = messages[-1]\n",
        "        else:\n",
        "            raise ValueError(\"No message found in input\")\n",
        "        outputs = []\n",
        "        for tool_call in message.tool_calls:\n",
        "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
        "                tool_call[\"args\"]\n",
        "            )\n",
        "            outputs.append(\n",
        "                ToolMessage(\n",
        "                    content=json.dumps(tool_result),\n",
        "                    name=tool_call[\"name\"],\n",
        "                    tool_call_id=tool_call[\"id\"],\n",
        "                )\n",
        "            )\n",
        "        return {\"messages\": outputs}\n",
        "\n",
        "\n",
        "tool_node = BasicToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b049afc4-7757-40ba-8e00-589d378e816d",
      "metadata": {
        "id": "b049afc4-7757-40ba-8e00-589d378e816d"
      },
      "source": [
        "### Define `conditional_edges`\n",
        "\n",
        "Recall that **edges** route the control flow from one node to the next. **Conditional edges** usually contain \"if\" statements to route to different nodes depending on the current graph state. These functions receive the current graph `state` and return a string or list of strings indicating which node(s) to call next.\n",
        "\n",
        "Below, call define a router function called `route_tools`, that checks for tool_calls in the chatbot's output. Provide this function to the graph by calling `add_conditional_edges`, which tells the graph that whenever the `chatbot` node completes to check this function to see where to go next.\n",
        "\n",
        "The condition will route to `action` if tool calls are present and \"`__end__`\" if not.\n",
        "\n",
        "Later, we will replace this with the prebuilt [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#tools_condition) to be more concise, but implementing it ourselves first makes things more clear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d662df94-66ac-4c6c-92f0-4c93620f1c74",
      "metadata": {
        "id": "d662df94-66ac-4c6c-92f0-4c93620f1c74"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "\n",
        "def route_tools(\n",
        "    state: State,\n",
        ") -> Literal[\"tools\", \"__end__\"]:\n",
        "    \"\"\"Use in the conditional_edge to route to the ToolNode if the last message\n",
        "\n",
        "    has tool calls. Otherwise, route to the end.\"\"\"\n",
        "    if isinstance(state, list):\n",
        "        ai_message = state[-1]\n",
        "    elif messages := state.get(\"messages\", []):\n",
        "        ai_message = messages[-1]\n",
        "    else:\n",
        "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
        "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
        "        return \"tools\"\n",
        "    return \"__end__\"\n",
        "\n",
        "\n",
        "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"__end__\" if\n",
        "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    route_tools,\n",
        "    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
        "    # It defaults to the identity function, but if you\n",
        "    # want to use a node named something else apart from \"tools\",\n",
        "    # You can update the value of the dictionary to something else\n",
        "    # e.g., \"tools\": \"my_tools\"\n",
        "    {\"tools\": \"tools\", \"__end__\": \"__end__\"},\n",
        ")\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2aa67c2-dd1b-4bf2-8c64-eea44296d15f",
      "metadata": {
        "id": "a2aa67c2-dd1b-4bf2-8c64-eea44296d15f"
      },
      "source": [
        "**Notice** that conditional edges start from a single node. This tells the graph \"any time the '`chatbot`' node runs, either go to 'action' if it calls a tool, or end the loop if it responds directly.\n",
        "\n",
        "The prebuilt `tools_condition` returns the \"`__end__`\" string if no tool calls are made. When the graph transitions to `__end__`, it has no more tasks to complete and ceases execution. Because the condition can return `__end__`, we don't need to explicitly set a `finish_point` this time. Our graph already has a way to finish!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Graph\n",
        "\n",
        "Let's visualize the graph we've built. The following function has some additional dependencies to run that are unimportant for this tutorial."
      ],
      "metadata": {
        "id": "MbAjtMJLBNX8"
      },
      "id": "MbAjtMJLBNX8"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8b49509c-9d97-457c-a76a-c495fb30ccbc",
      "metadata": {
        "id": "8b49509c-9d97-457c-a76a-c495fb30ccbc",
        "outputId": "ba952ded-54af-4064-b0e7-70464eedc963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c59593ef-5073-4279-931e-828dae971f23",
      "metadata": {
        "id": "c59593ef-5073-4279-931e-828dae971f23"
      },
      "source": [
        "### Run Graph\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "051dc374-67cc-4371-9dd1-221e07593148",
      "metadata": {
        "id": "051dc374-67cc-4371-9dd1-221e07593148",
        "outputId": "f86f4b95-4694-4980-fe2b-91d40516e644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: how much data throughput does the ethereum blockchain do ?\n",
            "Assistant: The Ethereum blockchain can process a theoretical maximum of around 15 transactions per second, but in practice, it is often much less than that due to network congestion and other factors. In terms of data throughput, this translates to a few hundred kilobytes of data per block, with each block being added to the blockchain approximately every 15 seconds. However, it's important to note that the actual data throughput can vary widely depending on the state of the network and the complexity of the transactions being processed.\n",
            "User: what is a good blockchain to use if you want to record json strings and interact with web apis?\n",
            "Assistant: \n",
            "Assistant: [{\"url\": \"https://www.zeeve.io/blog/indexed-blockchain-data-vs-json-rpc-apis-for-web3-applications/\", \"content\": \"JSON RPC APIs and indexed blockchain data are the two widely preferred and feasible options for retrieving blockchain data in a transparent, secure, and immutable manner. Through this comprehensive guide, we will dive into a comparative analysis of Indexed Blockchain Data vs. JSON RPC APIs, allowing you to understand which one is the perfect ...\"}, {\"url\": \"https://www.moesif.com/blog/api-product-management/api-analytics/Top-8-Blockchain-APIs-For-Developers/\", \"content\": \"BlockCypher API. The BlockCypher API is one of the most popular blockchain APIs available today. It offers a simple interface that makes it easy to get started with blockchain development. allowing the developer to interact with Bitcoin, Etherum, Litecoin, and Dogecoin on a variety of platforms.\"}]\n",
            "Assistant: Based on the information provided by the tool, BlockCypher API is one of the popular blockchain APIs available today. It offers a simple interface and allows interaction with Bitcoin, Ethereum, Litecoin, and Dogecoin on various platforms. This API can be useful for recording JSON strings and interacting with web APIs in a blockchain.\n",
            "User: q\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
        "        for value in event.values():\n",
        "            if isinstance(value[\"messages\"][-1], BaseMessage):\n",
        "                print(\"Assistant:\", value[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add 2markdown Node\n",
        "\n"
      ],
      "metadata": {
        "id": "EaXhl84ykYd8"
      },
      "id": "EaXhl84ykYd8"
    },
    {
      "cell_type": "code",
      "source": [
        "# configure the api\n",
        "import getpass\n",
        "markdown_api_key = getpass.getpass(\"2Markdown api_key\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Mf_kvXtpau1",
        "outputId": "704e47ce-4540-4041-9ac4-adccda1a1bfa"
      },
      "id": "0Mf_kvXtpau1",
      "execution_count": 52,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2Markdown api_key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import ToMarkdownLoader\n",
        "\n",
        "url = \"https://www.moesif.com/blog/api-product-management/api-analytics/Top-8-Blockchain-APIs-For-Developers/\"\n",
        "\n",
        "loader = ToMarkdownLoader(url=url, api_key=markdown_api_key)\n",
        "docs = loader.load()\n",
        "print(docs[0].page_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixBa7sFikWki",
        "outputId": "e97331f3-d5b4-4d68-c9dd-e1f28115dcd5"
      },
      "id": "ixBa7sFikWki",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Developers are always looking for new ways to make their applications more secure and efficient. Blockchain APIs are one way to do this. A blockchain API is an application programming interface that allows developers to interact with a blockchain. By using a blockchain API, developers can access the data and functionality of a blockchain without having to build their own blockchain platform. This can save time and resources, as well as provide a more secure environment for development. In this post, we will explore the best blockchain APIs for developers. We will look at the features and benefits of each blockchain API, as well as how they can be used to create more secure and efficient decentralized applications.\n",
            "\n",
            "## What Is Blockchain?\n",
            "\n",
            "A blockchain is a digital ledger of all cryptocurrency transactions. It is constantly growing as “completed” blocks are added to it with a new set of recordings. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data. Bitcoin nodes use the block chain to differentiate legitimate Bitcoin transactions from attempts to re-spend coins that have already been spent elsewhere.\n",
            "\n",
            "The API allows the blockchain developer to interact with the blockchain in a variety of ways. For example, they can [develop digital wallets](https://www.antino.com/blog/ewallet-app-development) for users, send and receive payments, and check balances. The API also enables developers to monitor markets and trends, as well as create applications that can be used to track prices or manage investments.\n",
            "\n",
            "The most popular blockchain is the one that powers Bitcoin, but there are many other types of blockchains being developed for a variety of purposes. Some of these include Ethereum, Litecoin, and Monero.\n",
            "\n",
            "## What Are the Benefits of Using Blockchain APIs?\n",
            "\n",
            "Blockchain APIs offer a number of benefits for developers. They can help to [streamline the process](https://julianlankstead.com/streamlining-business-processes) of developing applications and make it easier to integrate with other systems. In addition, Blockchain APIs can provide access to data that is stored on the blockchain, making it easier for developers to create applications that make use of this crypto data.\n",
            "\n",
            "Blockchain APIs can also help to reduce the costs associated with developing applications. By making it easier to access data stored on the blockchain, developers can avoid the need to build their own infrastructure to support their decentralized application. This can lead to significant savings in both time and money.\n",
            "\n",
            "In addition, Blockchain APIs can provide developers with a way to monetize their applications. By charging for access to data or functionality provided by an API, developers can generate revenue from their applications. This can help to offset the costs of developing and maintaining a decentralized application.\n",
            "\n",
            "Finally, Blockchain APIs can help to create a more open and accessible ecosystem for applications. By making it easier for developers to access data and functionality provided by other applications, Blockchain APIs can help to create an environment where applications can interoperate with each other. This can lead to new and innovative app development being created that would not be possible without the use of APIs.\n",
            "\n",
            "## The Best Blockchain APIs for Blockchain Development\n",
            "\n",
            "There are a number of different blockchain APIs available for developers, each with its own advantages and disadvantages. In this article, we’ll take a look at some of the best blockchain APIs currently available and provide a brief overview of each one.\n",
            "\n",
            "### BlockCypher API\n",
            "\n",
            "The **BlockCypher API** is one of the most popular blockchain APIs available today. It offers a simple interface that makes it easy to get started with blockchain development. allowing the developer to interact with Bitcoin, Etherum, Litecoin, and Dogecoin on a variety of platforms. The versatile development tools gives you the ability to interact with a smart contract, get notified about an unconfirmed transaction or create a multi signature transaction. Other features include:\n",
            "\n",
            "- Data Addresses. Transactions, Blocks, Smart contracts.\n",
            "- Interactions. Create Transactions. Decode transactions. Interact with contracts. Deploy contracts.\n",
            "- Notifications. Transactions webhook. Block webhook. Double spend webhook. Websocket.\n",
            "- Advanced Features, Multisig. Segwit Support. Confidence factor.\n",
            "\n",
            "### Chain API\n",
            "\n",
            "The **Chain API** is another popular option for blockchain developers. It offers a more comprehensive set of features than BlockCypher, making it a good choice for more experienced developers. Chain also has good documentation and support for multiple programming languages. ChainAPI has a user-friendly interface that will allow API providers ro set up first-party oracles easily. Interface is used by the API2 employees to do integrations on behalf of API providers with necessary greatly improving their efficiency and correctness. Chain API will make it easier for API providers and requesters to interact with the Airnode protocol across multiple chains, including a node dashboard.\n",
            "\n",
            "### CoinBase API\n",
            "\n",
            "The **CoinBase API** can be a great alternative option for blockchain development. Coinbase Pro provides an API that makes it easy to carry out various tasks. This includes sourcing real-time prices, storing digital currency safely, buying or selling cryptocurrencies, and processing digital wallets. They also offer a premium option with a much more advanced API for your blockchain solution. Some of the features CoinBase API offers:\n",
            "\n",
            "- Generate bitcoin cash wallets and addresses.\n",
            "- Securely store the coins.\n",
            "- Obtain real-time and/pr historical price data.\n",
            "- Get notified when the payments arrive.\n",
            "- Send/receive or sell/buy bitcoin cash, bitcoin, litecoin and ethereum.\n",
            "\n",
            "### Crypto API\n",
            "\n",
            "**Crypto APIs** is a blockchain infrastructure provider that makes developing and managing Web 3 solutions easy and efficient. They provide Wallet as a Service (WaaS), Blockchain Data, Blockchain Events, Blockchain Automations, Blockchain Tools, and Market Data to make development easier. Developers use their SDK to access over 100 end points from a single provider for diverse solutions including digital banks, exchanges, wallets, custodians, lending products and more. Chainlink, Ledger, Nexo and Paypal have already tried Crypto APIs’ company’s REST APIs. They love the white-glove support and features that are effective fast. Blockchain developers appreciate how reliable they are. Some features include….\n",
            "\n",
            "- MPC’s Wallet as a Service is the best digital wallet on the market - it incorporates the top features, protection, and authorization process currently available.\n",
            "- Blockchain Data- Unified access to complex and dynamic data from a single point using REST APIs.\n",
            "- With Node as a Service, you’ll be able to quickly deploy your blockchain technology with shared or dedicated node infrastructure. Utilizing JSON-RPC, it can be plugged into existing infrastructure and allows for an easy development environment in Javascript.\n",
            "- Blockchain Automations - automatically forward any coins or tokens that are received to a preferred main deposit address.\n",
            "\n",
            "### Blockchain API\n",
            "\n",
            "The **Blockchain API** is an additional option worth considering for blockchain development. Blockchain API is the perfect solution for providing cryptocurrency payments to your projects. The high quality of their services and capabilities make it easy to integrate. Blockchain API has been successful in integrating with over 25,000 developers. They offer many different APIs that are tailored to satisfy the needs of different customers. They have APIs for wallets, payment processing, querying data, exploring the blockchain network, analyzing crypto data and more.\n",
            "\n",
            "It has several aspects that make it a competitive provider in the market. This includes data storage in blockchain, which is done in block form. The result of this is JSON data, which deals with transactions. Blockchain’s offline-first approach also means that they don’t need additional cryptocurrency storage services. It has a vast developer community and low timeouts, as well as an accessible JSON data format. You can also access the blockchain network through e-wallet accounts.\n",
            "\n",
            "### Block.io API\n",
            "\n",
            "The **Block.io API** offers a simple interface that makes it easy to get started with blockchain development. Block.io also provides support for multiple programming languages, making it a good choice for developers who want to write code in their language of choice. Don’t forget to test your application thoroughly and always keep your private key confidential.\n",
            "\n",
            "### BitPay API\n",
            "\n",
            "The **BitPay API** an international digital asset, BitPay’s API allows you to perform a wide range of tasks. BitPay provides a standards-based REST interface that enables application developers to interact in powerful and secure ways with their BitPay account. Using the BitPay API, clients can manage invoices, issue refunds, view merchant records, and more.Developers may choose to call the API over HTTPS using the language of their choice, or take advantage of our code libraries. And, if their preferred language isn’t listed, they can still customize the integration.\n",
            "\n",
            "### GetBlock API\n",
            "\n",
            "The **GetBlock API** is another popular choice for a blockchain developer to explore. GetBlock provides a simple, easy way to use the power of blockchains. Discover their high-speed running nodes and secured access to API for bitcoin and Binance’s Smart Chain on blockchains, like Bitcoin, that allow you to run a decentralized app efficiently. GetBlock offers API, smart contract, and explorer data services. It also provides a blockchain development program which has access to raw data. When you use our service, you are guaranteed speedy access over blocks, transactions, and contracts that can be achieved by simply using the base API data. GetBlock API has helpful technical guides and documents and offers custom SLAs that are tailored solutions to suit your business needs.\n",
            "\n",
            "### Factors to consider when determining on a blockchain API\n",
            "\n",
            "Developers and development teams have preferences when it comes to choosing the best programming languages, architecture patterns, frameworks, or libraries. Blockchain API’s are similar.\n",
            "\n",
            "**Technology**: It is important to use open-source codes which help avoid mistakes and improve overall security as it can be tested by other developers.\n",
            "\n",
            "**Performance**: Blockchain APIs vary in performance and capacity, so it’s important to pick one that meets the needs of your application. A few transactions per second may be enough if you just need to encrypt a couple of documents, but apps with thousands of transactions per second will require subsystems called microservices. These will allow for handling high loads and continued responsiveness when processing user requests.\n",
            "\n",
            "**Compatibility**: You’ll want to make sure the API you choose can support the coins you need.\n",
            "\n",
            "## How To Use Blockchain APIs\n",
            "\n",
            "If you’re a blockchain developer looking to get started with blockchain technology, one of the first things you’ll need is a blockchain API. This will allow you to interact with the blockchain and build your blockchain application on top of it.\n",
            "\n",
            "There are many different blockchain APIs available, so it’s important to choose one that’s right for your needs. Some factors to consider include the programming language you’re using, the features you need, and the level of development tools offered.\n",
            "\n",
            "Once you’ve chosen a blockchain API, you’ll need to register for an account and get an API key. Then, you can start building your app development.\n",
            "\n",
            "Be sure to test your application thoroughly before deploying it. And, always monitor the blockchain for changes that could affect your application. Tools such as Moesif can help leverage a browser SDK to access API call data from the client side. From there, we can generate debugging and monitoring reports to ensure your site’s running smoothly. If something does go wrong, we’ll alert you immediately so that it doesn’t have time to expand into a larger problem.\n",
            "\n",
            "### Support and Monitoring with Moesif\n",
            "\n",
            "Moesif has unique features that allow you to associate each event that occurs in your app with a user and/or company. This gives a more nuanced understanding of the developer experience and how APIs are being used by each individual. A perfect example of this is Funnels and Retention.\n",
            "\n",
            "[Funnels](https://www.moesif.com/docs/user-analytics/conversion-funnel-analysis/?utm_campaign=Int-site&utm_source=blog&utm_medium=body-cta&utm_content=top-8-blockchains) are a step-by-step breakdown of a flow within your app. For example, you might be optimizing your sign-up process and looking for ways to speed it up. This analysis provides you with insights and how long it takes users to make their way through the steps and what percentage of customers complete them. This is important, as it helps establish a baseline you can use as you try to improve conversions and API usage.\n",
            "\n",
            "[![A Funnel chart created within Moesif that shows a step-by-step breakdown of a flow within your app](https://www.moesif.com/blog/images/posts/2022-11-23-Top-8-Blockchain-APIs-For-Developers/IMGFunnelBlockchain.png)](https://www.moesif.com/solutions/api-product-management/?utm_campaign=Int-site&utm_source=blog&utm_medium=body-cta&utm_content=top-8-blockchains)\n",
            "\n",
            "A [retention analysis](https://www.moesif.com/docs/user-analytics/cohort-retention-analysis/?utm_campaign=Int-site&utm_source=blog&utm_medium=body-cta&utm_content=top-8-blockchains) can help determine when customers are quitting or becoming inactive with your product and APIs. This can help to improve retention and allow you to know when there is a problem with your developer experience. This will enable you to establish baselines to correlate improvements or deterioration of your developer experience with the retention of users.\n",
            "\n",
            "[![A Retention analysis chart that can help determine when customers are quitting or becoming inactive with your product and APIs](https://www.moesif.com/blog/images/posts/2022-11-23-Top-8-Blockchain-APIs-For-Developers/IMGRentionBlockcahin.png)](https://www.moesif.com/solutions/api-product-management/?utm_campaign=Int-site&utm_source=blog&utm_medium=body-cta&utm_content=top-8-blockchains)\n",
            "\n",
            "Another way to track your API usage is to use alerts to let you now when users are having difficulties or to let you know about anomalies in traffic. For example, an alert might be used to let a customer success team know about issues that users are experiencing so they can reach out and help. Alerts also allow product teams to learn from feedback on issues in the short-term and make improvements in the future. Alerts are made up of two types: Static and Dynamic.\n",
            "\n",
            "Static alerts can be set as a specific threshold, like having **more than 3 401 - Unauthorized errors in an hour**. Dynamic alerts allow you to set a threshold such as noticing when a user is **experiencing a surge in 401 errors compared to their average amount**. It allows you to monitor trends or spikes without putting a specific number in for the threshold.\n",
            "\n",
            "Both forms of alerts can improve the user experience within your product, especially if it’s high quality and supportive from its team members. For more info on how to create an alert in Moesif, check out our docs.\n",
            "\n",
            "Moesif allows you to set up automated email flows based on events your users are experiencing. This is done through creating [behavioral emails](https://www.moesif.com/docs/behavioral-emails/?utm_campaign=Int-site&utm_source=blog&utm_medium=body-cta&utm_content=top-8-blockchains). For instance, if a user has a high number of **401 - Unauthorized** responses then you may want to send them an email with suggestions on how to fix the issue or a guide. This takes the pressure off the support team and also makes it possible for instant and proactive action to be taken.\n",
            "\n",
            "### Enabling API Monetization with Moesif\n",
            "\n",
            "As an API provider, you may want to create some type of revenue with your APIs. When first starting the monetization process, you may find that the challenges are steep and complex. To solve these issues and create a smooth experience, significant customization and testing are going to be required. Thankfully, Moesif is a robust API monetization platform that can help. With unlimited options available through the platform, it’s easy for you as an API provider to drive adoption and revenue from your APIs.\n",
            "\n",
            "With Moesif, you will have the flexibility to make well-informed decisions to increase revenue and profits. You’ll also have other tools that can supplement your user journey and improve your API product.\n",
            "\n",
            "With Moesif, usage data can be synced to the provider. This calculated API usage is sent to the billing provider and is used to generate invoices for users. Alternatively, you can support pre-paid billing, where a developer buys credits as they need them.\n",
            "\n",
            "[![A diagram showcasing the integration between Moesif and API payment platforms Recurly, Stripe, and Chargebee](https://www.moesif.com/blog/images/posts/2022-11-23-Top-8-Blockchain-APIs-For-Developers/IMGAPIMoneBlockchain.png)](https://www.moesif.com/solutions/metered-api-billing/?utm_campaign=Int-site&utm_source=blog&utm_medium=body-cta&utm_content=top-8-blockchains)\n",
            "\n",
            "For more info on how to monetize your APIs in Moesif, check out [our docs](https://www.moesif.com/docs/?utm_campaign=Int-site&utm_source=blog&utm_medium=body-cta&utm_content=top-8-blockchains).\n",
            "\n",
            "### Customer Success using Moesif\n",
            "\n",
            "Leading customer success for API-first or developer-focused businesses is much different from traditional enterprise software, The best API products are built in a way that make them easy to use and hands-off, meaning customers don’t have to log in to the platform once it’s implemented.\n",
            "\n",
            "When customer success works with API-first businesses, they should be prepared for two different experiences with their customers. The first experience is when the customer signs up and logs into your portal. Second, there is the customer’s experience when integrating with and interaction with your API platform. Both are considered to be part of the onboarding experience, and metrics should be recorded for each one. What’s more, it is the API that has the potential to offer long-term value. Customers who sign up but never fully integrate with or use that platform can churn quickly (if they’re paying already).\n",
            "\n",
            "In Moesif, every API call can be attributed to a user and/or company. Moesif creates a profile for each of these companies and users which allows customer success teams to have a CRM-style view of the customers attributes.\n",
            "\n",
            "[![A workspace within Moesif showcasing multiple Customer Success centric dashboards](https://www.moesif.com/blog/images/posts/2022-11-23-Top-8-Blockchain-APIs-For-Developers/IMGDashboardBlockchain.png)](https://www.moesif.com/features/api-dashboards/?utm_campaign=Int-site&utm_source=blog&utm_medium=body-cta&utm_content=top-8-blockchains)\n",
            "\n",
            "Customer success teams can also easily track customer interaction with the platform and keep an eye on key customer health metrics. These metrics will show under the user or companies profile dashboard. Having the ability to see customer metrics in a visual dashboard which shows customer usage, including errors and integration problems that may be occurring.\n",
            "\n",
            "[![A mockup of a Profile View within Moesif showcasing statistics specific to a given Company](https://www.moesif.com/blog/images/posts/2022-11-23-Top-8-Blockchain-APIs-For-Developers/IMGCompanydashboardBlockchain.png)](https://www.moesif.com/features/api-dashboards/?utm_campaign=Int-site&utm_source=blog&utm_medium=body-cta&utm_content=top-8-blockchains)\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "The Blockchain APIs on this list provide a wide range of capabilities, from allowing developers to create their own digital currency wallets, to helping them interact with smart contracts on the Ethereum blockchain. While some of these APIs are still in development, and therefore may not be as reliable as others, they all show great promise for the future of blockchain technology. In the meantime, we encourage you to experiment with all of these APIs to see which one(s) work best for your project.\n",
            "\n",
            "When creating APIs for yourself, using a solution like Moesif to monitor and improve your users API experience is crucial. The adoption of APIs can be easily tracked and enhanced using many of the features talked about in this article. The features we covered included [API analytics](https://www.moesif.com/docs/api-analytics/?utm_campaign=Int-site&utm_source=blog&utm_medium=body-cta&utm_content=top-8-blockchains), [API monitoring](https://www.moesif.com/docs/api-monitoring/?utm_campaign=Int-site&utm_source=blog&utm_medium=body-cta&utm_content=top-8-blockchains), and [API metered billing](https://www.moesif.com/docs/metered-billing/?utm_campaign=Int-site&utm_source=blog&utm_medium=body-cta&utm_content=top-8-blockchains). To try out these features for yourself for your own Web3 APIs, log in to Moesif or [sign up](https://www.moesif.com/signup?utm_campaign=Int-site&utm_source=blog&utm_medium=body-cta&utm_content=top-8-blockchains) today to get started.\n",
            "\n",
            "![Learn More About Moesif](https://blog.moesif.com/images/posts/cta/logo.svg)\n",
            "Enforce Usage Quotas with Moesif\n",
            "\n",
            "14 day free trial. No credit card required.\n",
            "[Learn More](https://www.moesif.com/features/api-governance-rules?utm_campaign=Int-site&utm_source=blog&utm_medium=body-cta&utm_content=top-8-blockchains)\n",
            "\n",
            "![](https://blog.moesif.com/images/posts/cta/cta-monitoring.svg)\n",
            "\n",
            "![Preet Kaur](https://blog.moesif.com/images/authors/preet.jpg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89da9e85-2e5d-49c2-8cbd-572cbdb89135",
      "metadata": {
        "id": "89da9e85-2e5d-49c2-8cbd-572cbdb89135"
      },
      "source": [
        "### Snippet\n",
        "\n",
        "- replacing our `BasicToolNode` for the prebuilt [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode), and our `route_tools` condition with the prebuilt [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#tools_condition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "8755d551-160e-4f8f-afac-0e4e07ca79ff",
      "metadata": {
        "id": "8755d551-160e-4f8f-afac-0e4e07ca79ff"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, Union\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "from langchain import hub\n",
        "from langchain.agents import create_openai_tools_agent\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults, TavilyAnswer\n",
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "llm = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae45f2aa-396f-4f3f-848b-7750611617f8",
      "metadata": {
        "id": "ae45f2aa-396f-4f3f-848b-7750611617f8"
      },
      "source": [
        "## Part 3: Add Memory to Chatbot\n",
        "\n",
        "Our chatbot can now use tools to answer user questions, but it doesn't remember the context of previous interactions. This limits its ability to have coherent, multi-turn conversations.\n",
        "\n",
        "LangGraph solves this problem through **persistent checkpointing**. If you provide a `checkpointer` when compiling the graph and a `thread_id` when calling your graph, LangGraph automatically saves the state after each step. When you invoke the graph again using the same `thread_id`, the graph loads its saved state, allowing the chatbot to pick up where it left off.\n",
        "\n",
        "We will see later that **checkpointing** is _much_ more powerful than simple chat memory - it lets you save and resume complex state at any time for error recovery, human-in-the-loop workflows, time travel interactions, and more. But before we get too ahead of ourselves, let's add checkpointing to enable multi-turn conversations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Create `SqliteSaver` checkpointer"
      ],
      "metadata": {
        "id": "TZiNnxCOEMn8"
      },
      "id": "TZiNnxCOEMn8"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "6baafdf6-6803-4305-9381-9dc970468a4d",
      "metadata": {
        "id": "6baafdf6-6803-4305-9381-9dc970468a4d"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "memory = SqliteSaver.from_conn_string(\":memory:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08d3d11a-1b42-4cbb-8e11-2a4294263d90",
      "metadata": {
        "id": "08d3d11a-1b42-4cbb-8e11-2a4294263d90"
      },
      "source": [
        "**Notice** that we've specified `:memory` as the Sqlite DB path. This is convenient for our tutorial (it saves it all in-memory). In a production application, you would likely change this to connect to your own DB and/or use one of the other checkpointer classes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Define Graph\n",
        "\n",
        "Now that you've already built your own `BasicToolNode`, we'll replace it with LangGraph's prebuilt `ToolNode` and `tools_condition`, since these do some nice things like parallel API execution. Apart from that, the following is all copied from Part 2."
      ],
      "metadata": {
        "id": "s9TLoNs7Eggg"
      },
      "id": "s9TLoNs7Eggg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6a51f1e-00de-4701-8931-de8cf19294ae",
      "metadata": {
        "id": "e6a51f1e-00de-4701-8931-de8cf19294ae"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, Union\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.messages import BaseMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "llm = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.set_entry_point(\"chatbot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a292dfe-764f-4561-90aa-71317d679d3e",
      "metadata": {
        "id": "8a292dfe-764f-4561-90aa-71317d679d3e"
      },
      "source": [
        "### Compile Graph with checkpointer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a06548bf-81fa-4436-b4c1-f68601fb4187",
      "metadata": {
        "id": "a06548bf-81fa-4436-b4c1-f68601fb4187"
      },
      "outputs": [],
      "source": [
        "graph = graph_builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df01805c-4458-4474-b13b-59ecfe228f12",
      "metadata": {
        "id": "df01805c-4458-4474-b13b-59ecfe228f12"
      },
      "source": [
        "### Visualize Graph\n",
        "\n",
        "Notice the connectivity of the graph hasn't changed since Part 2. All we are doing is checkpointing the `State` as the graph works through each node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "761d15fb-d5e2-4d50-a630-126d77e77294",
      "metadata": {
        "id": "761d15fb-d5e2-4d50-a630-126d77e77294",
        "outputId": "1e0835dd-9a79-4711-8e0b-b76340cfeca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c8265ef-e5b4-4c32-9856-5572b5652142",
      "metadata": {
        "id": "2c8265ef-e5b4-4c32-9856-5572b5652142"
      },
      "source": [
        "### Configure Chatbot\n",
        "\n",
        "Now you can interact with your bot! First, pick a thread to use as the key for this conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "be7b5abb-04ef-4d53-83d1-d4d3139cc43a",
      "metadata": {
        "id": "be7b5abb-04ef-4d53-83d1-d4d3139cc43a"
      },
      "outputs": [],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0b1a5ee-7fa2-475c-a9db-749694b90ba9",
      "metadata": {
        "id": "d0b1a5ee-7fa2-475c-a9db-749694b90ba9"
      },
      "source": [
        "### Run Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "dba1b168-f8e0-496d-9bd6-37198fb4776e",
      "metadata": {
        "id": "dba1b168-f8e0-496d-9bd6-37198fb4776e",
        "outputId": "61283d79-e29b-44f1-ef63-0e5445ca10e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi there! My name is Will.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Will! How can I help you today? I can use tools to find information or answer questions to the best of my ability. If I don't need to use a tool, I'll go ahead and provide a direct response. Let's get started!\n"
          ]
        }
      ],
      "source": [
        "user_input = \"Hi there! My name is Will.\"\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33c6b470-5082-4c3e-b732-34de47c88735",
      "metadata": {
        "id": "33c6b470-5082-4c3e-b732-34de47c88735"
      },
      "source": [
        "**Note:** The config was provided as the **second positional argument** when calling our graph. It importantly is _not_ nested within the graph inputs (`{'messages': []}`).\n",
        "\n",
        "Let's ask a followup: see if it remembers your name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "f5447778-53d7-47f3-801b-f47bcf2185a0",
      "metadata": {
        "id": "f5447778-53d7-47f3-801b-f47bcf2185a0",
        "outputId": "7ac301db-77cb-4262-eb55-149cc16efddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Remember my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Yes, of course! You mentioned that your name is Will. I'm here to help you with any questions or information you need. If I don't know the answer, I'll do my best to find the information using the tools at my disposal. Is there something specific you'd like to know?\n"
          ]
        }
      ],
      "source": [
        "user_input = \"Remember my name?\"\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33be4cd8-f96f-4949-9d1f-48054502e5d0",
      "metadata": {
        "id": "33be4cd8-f96f-4949-9d1f-48054502e5d0"
      },
      "source": [
        "**Notice** that we are't the memory using an external list: it's all handled by the checkpointer! You can inspect the full execution in this [LangSmith trace](https://smith.langchain.com/public/48387889-c002-47a8-9f6a-1f6b298db64b/r) to see what's going on.\n",
        "\n",
        "Don't believe me? Try this using a different config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "4527cf9a-b191-4bde-858a-e33a74a48c55",
      "metadata": {
        "id": "4527cf9a-b191-4bde-858a-e33a74a48c55",
        "outputId": "ba92ba63-cc3e-419e-c1ee-82c85ab78f31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Remember my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Of course, I remember that your name is not mentioned in the instructions. How can I assist you today? If you have a question that requires the use of a tool, I will make sure to follow the tool instructions. If not, I will respond directly without using a tool, as specified in the text instructions.\n"
          ]
        }
      ],
      "source": [
        "# The only difference is we change the `thread_id` here to \"2\" instead of \"1\"\n",
        "events = graph.stream(\n",
        "    {\"messages\": [(\"user\", user_input)]},\n",
        "    {\"configurable\": {\"thread_id\": \"2\"}},\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eeccbf0-ed74-4838-a7e9-31910d82b0b2",
      "metadata": {
        "id": "5eeccbf0-ed74-4838-a7e9-31910d82b0b2"
      },
      "source": [
        "**Notice** that the **only** change we've made is to modify the `thread_id` in the config. See this call's [LangSmith trace](https://smith.langchain.com/public/4647adf6-3835-4ce3-ba39-26ed4f167411/r) for comparison.\n",
        "\n",
        "\n",
        "### Inspect Graph State\n",
        "\n",
        "By now, we have made a few checkpoints across two different threads. But what goes into a checkpoint? To inspect a graph's `state` for a given config at any time, call `get_state(config)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "0be77c25-1423-4f2d-9b2d-28530cc761a4",
      "metadata": {
        "id": "0be77c25-1423-4f2d-9b2d-28530cc761a4",
        "outputId": "5e5289dd-3215-488d-bed2-cce38354e82c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content='Hi there! My name is Will.', id='4a1dc45c-6e70-49e0-8489-a7ad4a3d46fd'), AIMessage(content=\"Hello Will! How can I help you today? I can use tools to find information or answer questions to the best of my ability. If I don't need to use a tool, I'll go ahead and provide a direct response. Let's get started!\", response_metadata={'finish_reason': 'stop', 'logprobs': None, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'token_usage': {'completion_time': 0.097678048, 'completion_tokens': 55, 'prompt_time': 0.500506386, 'prompt_tokens': 1215, 'queue_time': None, 'total_time': 0.598184434, 'total_tokens': 1270}}, id='run-908cef1c-d21c-425c-88ce-1b1f0477c807-0'), HumanMessage(content='Remember my name?', id='d48766cc-832d-477d-a663-17a7ae65676d'), AIMessage(content=\"Yes, of course! You mentioned that your name is Will. I'm here to help you with any questions or information you need. If I don't know the answer, I'll do my best to find the information using the tools at my disposal. Is there something specific you'd like to know?\", response_metadata={'finish_reason': 'stop', 'logprobs': None, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'token_usage': {'completion_time': 0.117356092, 'completion_tokens': 66, 'prompt_time': 0.514069044, 'prompt_tokens': 1284, 'queue_time': None, 'total_time': 0.6314251359999999, 'total_tokens': 1350}}, id='run-4af16cc7-9ffc-43a5-8fec-e5f2417b2e14-0')]}, next=(), config={'configurable': {'thread_id': '1', 'thread_ts': '1ef18607-8002-6139-8004-6735cc3ae0d1'}}, metadata={'source': 'loop', 'step': 4, 'writes': {'chatbot': {'messages': [AIMessage(content=\"Yes, of course! You mentioned that your name is Will. I'm here to help you with any questions or information you need. If I don't know the answer, I'll do my best to find the information using the tools at my disposal. Is there something specific you'd like to know?\", response_metadata={'finish_reason': 'stop', 'logprobs': None, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'token_usage': {'completion_time': 0.117356092, 'completion_tokens': 66, 'prompt_time': 0.514069044, 'prompt_tokens': 1284, 'queue_time': None, 'total_time': 0.6314251359999999, 'total_tokens': 1350}}, id='run-4af16cc7-9ffc-43a5-8fec-e5f2417b2e14-0')]}}}, created_at='2024-05-22T17:26:48.097588+00:00', parent_config={'configurable': {'thread_id': '1', 'thread_ts': '1ef18607-6593-6096-8003-e3013ce2f422'}})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "snapshot = graph.get_state(config)\n",
        "snapshot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "c106bd09-f155-4e15-9120-c60c834106e5",
      "metadata": {
        "id": "c106bd09-f155-4e15-9120-c60c834106e5",
        "outputId": "97f416f6-424d-4649-94da-e25b26542f9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "()"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "snapshot.next  # (since the graph ended this turn, `next` is empty. If you fetch a state from within a graph invocation, next tells which node will execute next)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "627f4998-6780-4cce-8f3c-9a5580888e3a",
      "metadata": {
        "id": "627f4998-6780-4cce-8f3c-9a5580888e3a"
      },
      "source": [
        "The snapshot above contains the current state values, corresponding config, and the `next` node to process. In our case, the graph has reached an `__end__` state, so `next` is empty.\n",
        "\n",
        "LangGraph's checkpointing even handles **arbitrary complex graph states**, which is much more expressive and powerful than simple chat memory.\n",
        "  \n",
        "### Snippet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "9c50a794-3ae5-484c-8edd-50e0d54da982",
      "metadata": {
        "id": "9c50a794-3ae5-484c-8edd-50e0d54da982"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, Union\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.messages import BaseMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "llm = ChatGroq(temperature=0.1, model_name=\"mixtral-8x7b-32768\")\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "graph = graph_builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f1da240-ec9b-441f-9d47-44c6dc85d540",
      "metadata": {
        "id": "6f1da240-ec9b-441f-9d47-44c6dc85d540"
      },
      "source": [
        "## Part 4: Human-in-the-loop\n",
        "\n",
        "Agents can be unreliable and may need human input to successfully accomplish tasks. Similarly, for some actions, you may want to require human approval before running to ensure that everything is running as intended.\n",
        "\n",
        "LangGraph supports `human-in-the-loop` workflows in a number of ways. In this section, we will use LangGraph's `interrupt_before` functionality to always break the tool node.\n",
        "\n",
        "First, start from the snippet for Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "813505b2-18c1-46e9-b891-20a34232808b",
      "metadata": {
        "id": "813505b2-18c1-46e9-b891-20a34232808b"
      },
      "source": [
        "### compile with `interrupt_before`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0883e32-1a39-4ce9-ae32-bbd66708fd84",
      "metadata": {
        "id": "b0883e32-1a39-4ce9-ae32-bbd66708fd84"
      },
      "outputs": [],
      "source": [
        "graph = graph_builder.compile(\n",
        "    checkpointer=memory,\n",
        "    # This is new!\n",
        "    interrupt_before=[\"tools\"],\n",
        "    # Note: can also interrupt __after__ actions, if desired.\n",
        "    # interrupt_after=[\"tools\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run"
      ],
      "metadata": {
        "id": "5dAhrLj4Y1jQ"
      },
      "id": "5dAhrLj4Y1jQ"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "9f318020-ab7e-415b-a5e2-eddec6d9f3a6",
      "metadata": {
        "id": "9f318020-ab7e-415b-a5e2-eddec6d9f3a6",
        "outputId": "54b4d898-cc85-4c05-c619-250d1bd75015",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "I'm learning LangGraph. Could you do some research on it for me?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search_results_json (call_rx7p)\n",
            " Call ID: call_rx7p\n",
            "  Args:\n",
            "    query: LangGraph\n"
          ]
        }
      ],
      "source": [
        "user_input = \"I'm learning LangGraph. Could you do some research on it for me?\"\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39405637-13b1-40b1-a51e-6d60bf675ff1",
      "metadata": {
        "id": "39405637-13b1-40b1-a51e-6d60bf675ff1"
      },
      "source": [
        "### Inspect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "9bb7af46-9b4f-4bb1-b8b9-e9ddf7dbc82c",
      "metadata": {
        "id": "9bb7af46-9b4f-4bb1-b8b9-e9ddf7dbc82c",
        "outputId": "616c32ee-eb14-437b-9224-fcf1e2cefab3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tools',)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "snapshot = graph.get_state(config)\n",
        "snapshot.next"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89326046-2b11-4812-8b6d-8780306ec275",
      "metadata": {
        "id": "89326046-2b11-4812-8b6d-8780306ec275"
      },
      "source": [
        "**Notice** that unlike last time, the \"next\" node is set to **'action'**. We've interrupted here! Let's check the tool invocation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "3facda0a-e6ad-4b28-b627-753ad8c90c15",
      "metadata": {
        "id": "3facda0a-e6ad-4b28-b627-753ad8c90c15",
        "outputId": "91d94579-f7c5-4adf-abe4-cba27d8074a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'tavily_search_results_json',\n",
              "  'args': {'query': 'LangGraph'},\n",
              "  'id': 'call_rx7p'}]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "existing_message = snapshot.values[\"messages\"][-1]\n",
        "existing_message.tool_calls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a55a4c70-7226-4be0-8562-391f72bc1f2b",
      "metadata": {
        "id": "a55a4c70-7226-4be0-8562-391f72bc1f2b"
      },
      "source": [
        "This query seems reasonable. Nothing to filter here. The simplest thing the human can do is just let the graph continue executing. Let's do that below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Resume Graph by passing `None`\n",
        "Next, continue the graph! Passing in ==`None`== will just let the graph continue where it left off, without adding anything new to the state."
      ],
      "metadata": {
        "id": "DuPUckBXHChl"
      },
      "id": "DuPUckBXHChl"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "effb95d9-b7d5-40c5-9253-253d193b23b2",
      "metadata": {
        "id": "effb95d9-b7d5-40c5-9253-253d193b23b2",
        "outputId": "639f540e-bd83-476a-b4a9-066c87d7a9d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search_results_json\n",
            "\n",
            "[{\"url\": \"https://python.langchain.com/v0.1/docs/langgraph/\", \"content\": \"\\ud83e\\udd9c\\ud83d\\udd78\\ufe0fLangGraph. \\u26a1 Building language agents as graphs \\u26a1. Overview . LangGraph is a library for building stateful, multi-actor applications with LLMs. Inspired by Pregel and Apache Beam, LangGraph lets you coordinate and checkpoint multiple chains (or actors) across cyclic computational steps using regular python functions (or JS).The public interface draws inspiration from NetworkX.\"}, {\"url\": \"https://langchain-ai.github.io/langgraph/\", \"content\": \"LangGraph is framework agnostic (each node is a regular python function). It extends the core Runnable API (shared interface for streaming, async, and batch calls) to make it easy to: Seamless state management across multiple turns of conversation or tool usage. The ability to flexibly route between nodes based on dynamic criteria.\"}]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "LangGraph is a library for building stateful, multi-actor applications with LLMs (large language models). It allows you to coordinate and checkpoint multiple chains or actors across cyclic computational steps using regular python functions. LangGraph is framework agnostic, with each node being a regular python function. It extends the core Runnable API to make it easy for seamless state management across multiple turns of conversation or tool usage, and to flexibly route between nodes based on dynamic criteria. You can find more information on LangGraph at <https://python.langchain.com/v0.1/docs/langgraph/> and <https://langchain-ai.github.io/langgraph/>.\n"
          ]
        }
      ],
      "source": [
        "# `None` will append nothing new to the current state, letting it resume as if it had never been interrupted\n",
        "events = graph.stream(None, config, stream_mode=\"values\")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21e78a97-474f-4709-b51d-9d5e8323e14c",
      "metadata": {
        "id": "21e78a97-474f-4709-b51d-9d5e8323e14c"
      },
      "source": [
        "### Inspect\n",
        "\n",
        "Review this call's [LangSmith trace](https://smith.langchain.com/public/6a9012c0-bfa2-4fba-8dce-961d233f9512/r) to see the exact work that was done in the above call. Notice that the ==state== is loaded in the first step so that your chatbot can continue where it left off.\n",
        "\n",
        "**Congrats!** You've used an `interrupt` to add human-in-the-loop execution to your chatbot, allowing for human oversight and intervention when needed. This opens up the potential ==UIs== you can create with your AI systems. Since we have already added a **checkpointer**, the graph can be paused **indefinitely** and resumed at any time as if nothing had happened.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Snippet"
      ],
      "metadata": {
        "id": "WwLZwRm8ukZh"
      },
      "id": "WwLZwRm8ukZh"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "a7228caf-a5aa-4f68-b775-81ea5402aca8",
      "metadata": {
        "id": "a7228caf-a5aa-4f68-b775-81ea5402aca8"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, Union\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.messages import BaseMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "llm = ChatGroq(temperature=0.1, model_name=\"mixtral-8x7b-32768\")\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "\n",
        "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
        "graph = graph_builder.compile(\n",
        "    checkpointer=memory,\n",
        "    interrupt_before=[\"tools\"],\n",
        "    # Note: can also interrupt __after__ actions, if desired.\n",
        "    # interrupt_after=[\"tools\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6df38bc4-c177-4ccd-9ec2-83d32bf66722",
      "metadata": {
        "id": "6df38bc4-c177-4ccd-9ec2-83d32bf66722"
      },
      "source": [
        "## Part 5: Update State\n",
        "\n",
        "\n",
        "Thankfully, LangGraph lets you **manually update state**! Updating the state lets you control the agent's trajectory by modifying its actions (even modifying the past!). This capability is particularly useful when you want to correct the agent's mistakes, explore alternative paths, or guide the agent towards a specific goal.\n",
        "\n",
        "\n",
        "Begin with previous graph snippet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b3bcae-dd04-49da-a4ef-e05634657faf",
      "metadata": {
        "id": "a6b3bcae-dd04-49da-a4ef-e05634657faf"
      },
      "outputs": [],
      "source": [
        "snapshot = graph.get_state(config)\n",
        "existing_message = snapshot.values[\"messages\"][-1]\n",
        "existing_message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bf55a26-8c12-477a-9e83-5011d36ac4ee",
      "metadata": {
        "id": "3bf55a26-8c12-477a-9e83-5011d36ac4ee"
      },
      "source": [
        "So far, all of this is an _exact repeat_ of the previous section. The LLM just requested to use the search engine tool and our graph was interrupted. If we proceed as before, the tool will be called to search the web.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Mimic Tool Node with `update_state`\n",
        "\n",
        " with `ToolMessage` and `AIMessage`"
      ],
      "metadata": {
        "id": "tew5LcZ0IING"
      },
      "id": "tew5LcZ0IING"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a44bedc-ea91-4c22-976c-98b3d5a5e4a7",
      "metadata": {
        "id": "6a44bedc-ea91-4c22-976c-98b3d5a5e4a7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "\n",
        "answer = (\n",
        "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs.\"\n",
        ")\n",
        "new_messages = [\n",
        "    # The LLM API expects some ToolMessage to match its tool call. We'll satisfy that here.\n",
        "    ToolMessage(content=answer, tool_call_id=existing_message.tool_calls[0][\"id\"]),\n",
        "    # And then directly \"put words in the LLM's mouth\" by populating its response.\n",
        "    AIMessage(content=answer),\n",
        "]\n",
        "\n",
        "new_messages[-1].pretty_print()\n",
        "graph.update_state(\n",
        "    # Which state to update\n",
        "    config,\n",
        "    # The updated values to provide. The messages in our `State` are \"append-only\", meaning this will be appended\n",
        "    # to the existing state. We will review how to update existing messages in the next section!\n",
        "    {\"messages\": new_messages},\n",
        ")\n",
        "\n",
        "print(\"\\n\\nLast 2 messages;\")\n",
        "print(graph.get_state(config).values[\"messages\"][-2:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "584de971-6b10-4931-986e-cc35f7adbb3d",
      "metadata": {
        "id": "584de971-6b10-4931-986e-cc35f7adbb3d"
      },
      "source": [
        "Now the graph is complete, since we've provided the final response message! Since state updates simulate a graph step, they even generate corresponding traces. Inspec the [LangSmith trace](https://smith.langchain.com/public/c45207bb-bd26-4c9a-b631-928bbeebfbcb/r) of the `update_state` call above to see what's going on.\n",
        "\n",
        "The `update_state` function operates as if it were one of the nodes in your graph! By default, the update operation uses the node that was last executed, but you can manually specify it below. Let's add an update and tell the graph to treat it as if it came from the \"chatbot\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d16d95c3-b465-42ac-8015-26b669d45d1f",
      "metadata": {
        "id": "d16d95c3-b465-42ac-8015-26b669d45d1f"
      },
      "outputs": [],
      "source": [
        "graph.update_state(\n",
        "    config,\n",
        "    {\"messages\": [AIMessage(content=\"I'm an AI expert!\")]},\n",
        "    # Which node for this function to act as. It will automatically continue\n",
        "    # processing as if this node just ran.\n",
        "    as_node=\"chatbot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a1f0056-6b6f-425f-ac1a-0d4b0e9b85cc",
      "metadata": {
        "id": "5a1f0056-6b6f-425f-ac1a-0d4b0e9b85cc"
      },
      "source": [
        "### Inspect\n",
        "\n",
        "Check out the [LangSmith trace](https://smith.langchain.com/public/ce83989f-6e49-4bdd-bcd5-f54ca55c8d00/r/30b1406a-ae5b-4e9e-9fe5-032be6efb92e) for this update call at the provided link. **Notice** from the trace that the graph continues into the `tools_condition` edge. We just told the graph to treat the update `as_node=\"chatbot\"`. If we follow the diagram below and start from the `chatbot` node, we naturally end up in the `tools_condition` edge and then `__end__` since our updated message lacks tool calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4009ba6-dc0b-4216-ab0c-fbb104616f73",
      "metadata": {
        "id": "f4009ba6-dc0b-4216-ab0c-fbb104616f73"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96cd4ffa-8fb2-4bd6-bef9-564cbfe7e3ab",
      "metadata": {
        "id": "96cd4ffa-8fb2-4bd6-bef9-564cbfe7e3ab"
      },
      "source": [
        "Inspect the current state as before to confirm the checkpoint reflects our manual updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d420e813-a8c7-415d-ab31-5298d42491e4",
      "metadata": {
        "id": "d420e813-a8c7-415d-ab31-5298d42491e4"
      },
      "outputs": [],
      "source": [
        "snapshot = graph.get_state(config)\n",
        "print(snapshot.values[\"messages\"][-3:])\n",
        "print(snapshot.next)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "380222f4-65fa-4962-afe6-6a715fadb2de",
      "metadata": {
        "id": "380222f4-65fa-4962-afe6-6a715fadb2de"
      },
      "source": [
        "**Notice**: that we've continued to add AI messages to the state. Since we are acting as the `chatbot` and responding with an AIMessage that doesn't contain `tool_calls`, the graph knows that it has entered a finished state (`next` is empty).\n",
        "\n",
        "### Overwrite `messages`\n",
        "\n",
        "The [`add_messages`](https://langchain-ai.github.io/langgraph/reference/graphs/?h=add+messages#add_messages) function we used to annotate our graph's `State` above controls how updates are made to the `messages` key. This function looks at any message IDs in the new `messages` list. If the ID matches a message in the existing state, [`add_messages`](https://langchain-ai.github.io/langgraph/reference/graphs/?h=add+messages#add_messages) overwrites the existing message with the new content.\n",
        "\n",
        "As an example, let's update the tool invocation to make sure we get good results from our search engine! First, start a new thread:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fc99c7e-b61d-4aec-9c62-042798185ec3",
      "metadata": {
        "id": "9fc99c7e-b61d-4aec-9c62-042798185ec3"
      },
      "outputs": [],
      "source": [
        "user_input = \"I'm learning LangGraph. Could you do some research on it for me?\"\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}  # we'll use thread_id = 2 here\n",
        "events = graph.stream(\n",
        "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b019fc6-7826-4291-9178-6cecb5d7b3d0",
      "metadata": {
        "id": "8b019fc6-7826-4291-9178-6cecb5d7b3d0"
      },
      "source": [
        "### update the tool invocation\n",
        "\n",
        " for our agent. Maybe we want to search for human-in-the-loop workflows in particular."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7215533a-b7e2-4b2d-bc1d-5122b1d06b8b",
      "metadata": {
        "id": "7215533a-b7e2-4b2d-bc1d-5122b1d06b8b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "snapshot = graph.get_state(config)\n",
        "existing_message = snapshot.values[\"messages\"][-1]\n",
        "print(\"Original\")\n",
        "print(\"Message ID\", existing_message.id)\n",
        "print(existing_message.tool_calls[0])\n",
        "new_tool_call = existing_message.tool_calls[0].copy()\n",
        "new_tool_call[\"args\"][\"query\"] = \"LangGraph human-in-the-loop workflow\"\n",
        "new_message = AIMessage(\n",
        "    content=existing_message.content,\n",
        "    tool_calls=[new_tool_call],\n",
        "    # Important! The ID is how LangGraph knows to REPLACE the message in the state rather than APPEND this messages\n",
        "    id=existing_message.id,\n",
        ")\n",
        "\n",
        "print(\"Updated\")\n",
        "print(new_message.tool_calls[0])\n",
        "print(\"Message ID\", new_message.id)\n",
        "graph.update_state(config, {\"messages\": [new_message]})\n",
        "\n",
        "print(\"\\n\\nTool calls\")\n",
        "graph.get_state(config).values[\"messages\"][-1].tool_calls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "680f0ebd-ebce-4de6-8a9b-37d3d4ef0234",
      "metadata": {
        "id": "680f0ebd-ebce-4de6-8a9b-37d3d4ef0234"
      },
      "source": [
        "**Notice** that we've modified the AI's tool invocation to search for \"LangGraph human-in-the-loop workflow\" instead of the simple \"LangGraph\".\n",
        "\n",
        "Check out the [LangSmith trace](https://smith.langchain.com/public/cd7c09a6-758d-41d4-8de1-64ab838b2338/r) to see the state update call - you can see our new message has successfully updated the previous AI message.\n",
        "\n",
        "### Resume graph\n",
        "\n",
        " by streaming with an input of `None` and the existing config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a09bfc-3d90-4e54-878f-22e3cb28a418",
      "metadata": {
        "id": "03a09bfc-3d90-4e54-878f-22e3cb28a418"
      },
      "outputs": [],
      "source": [
        "events = graph.stream(None, config, stream_mode=\"values\")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "090b680b-f53f-4af2-a432-45f8c5a10779",
      "metadata": {
        "id": "090b680b-f53f-4af2-a432-45f8c5a10779"
      },
      "source": [
        "### Inspect\n",
        "\n",
        "Check out the [trace](https://smith.langchain.com/public/2d633326-14ad-4248-a391-2757d01851c4/r/6464f2f2-edb4-4ef3-8f48-ee4e249f2ad0) to see the tool call and later LLM response. **Notice** that now the graph queries the search engine using our updated query term - we were able to manually override the LLM's search here!\n",
        "\n",
        "All of this is reflected in the graph's checkpointed memory, meaning if we continue the conversation, it will recall all the _modified_ state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11d5b934-6d8b-4f52-a3bc-b3daa7207e00",
      "metadata": {
        "id": "11d5b934-6d8b-4f52-a3bc-b3daa7207e00"
      },
      "outputs": [],
      "source": [
        "events = graph.stream(\n",
        "    {\n",
        "        \"messages\": (\n",
        "            \"user\",\n",
        "            \"Remember what I'm learning about?\",\n",
        "        )\n",
        "    },\n",
        "    config,\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5166e1b-96a6-4ac0-88a1-bf32a422134a",
      "metadata": {
        "id": "a5166e1b-96a6-4ac0-88a1-bf32a422134a"
      },
      "source": [
        "**Congratulations!** You've used `interrupt_before` and `update_state` to manually modify the state as a part of a human-in-the-loop workflow. Interruptions and state modifications let you control how the agent behaves. Combined with persistent checkpointing, it means you can `pause` an action and `resume` at any point. Your user doesn't have to be available when the graph interrupts!\n",
        "\n",
        "The graph code for this section is identical to previous ones. The key snippets to remember are to add `.compile(..., interrupt_before=[...])` (or `interrupt_after`) if you want to explicitly pause the graph whenever it reaches a node. Then you can use `update_state` to modify the checkpoint and control how the graph should proceed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d88d4c9e-65c8-4093-a6c2-c261475f7c07",
      "metadata": {
        "id": "d88d4c9e-65c8-4093-a6c2-c261475f7c07"
      },
      "source": [
        "## Part 6: Customize State\n",
        "\n",
        "if you want to define complex behavior you can add additional fields to the state.\n",
        "\n",
        "In the examples above, we involved a human deterministically: the graph __always__ interrupted whenever an tool was invoked. Suppose we wanted our chat bot to have the choice of relying on a human.\n",
        "\n",
        "One way to do this is to create a passthrough \"human\" node, before which the graph will always stop. We will only execute this node if the LLM invokes a \"human\" tool. For our convenience, we will include an \"ask_human\" flag in our graph state that we will flip if the LLM calls this tool.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Define graph\n",
        " with an updated `State`"
      ],
      "metadata": {
        "id": "tgS7IguYK3Xb"
      },
      "id": "tgS7IguYK3Xb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf7e042-1718-4625-ae30-a9917f595449",
      "metadata": {
        "id": "3cf7e042-1718-4625-ae30-a9917f595449"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, Union\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.messages import BaseMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    # This flag is new\n",
        "    ask_human: bool"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e87f2cb8-c066-4b54-acc4-e8c7399c5f3d",
      "metadata": {
        "id": "e87f2cb8-c066-4b54-acc4-e8c7399c5f3d"
      },
      "source": [
        "### Define `RequestAssistance` Schema\n",
        "\n",
        "Next, define a schema to show the model to let it decide to request assistance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5192e54-6a28-42fe-a8a7-62d45d61f994",
      "metadata": {
        "id": "e5192e54-6a28-42fe-a8a7-62d45d61f994"
      },
      "outputs": [],
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "\n",
        "\n",
        "class RequestAssistance(BaseModel):\n",
        "    \"\"\"Escalate the conversation to an expert. Use this if you are unable to assist directly or if the user requires support beyond your permissions.\n",
        "\n",
        "    To use this function, relay the user's 'request' so the expert can provide the right guidance.\n",
        "    \"\"\"\n",
        "\n",
        "    request: str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b19c61b-2087-463b-adf8-96dbc193f41c",
      "metadata": {
        "id": "2b19c61b-2087-463b-adf8-96dbc193f41c"
      },
      "source": [
        "### Define Nodes\n",
        "The primary modification here is flip the `ask_human` flag if we see that the chat bot has invoked the `RequestAssistance` flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa59b266-14e5-4c75-8b3d-54fac28e8290",
      "metadata": {
        "id": "fa59b266-14e5-4c75-8b3d-54fac28e8290"
      },
      "outputs": [],
      "source": [
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
        "# We can bind the llm to a tool definition, a pydantic model, or a json schema\n",
        "llm_with_tools = llm.bind_tools(tools + [RequestAssistance])\n",
        "\n",
        "\n",
        "def chatbot(state: State):\n",
        "    response = llm_with_tools.invoke(state[\"messages\"])\n",
        "    ask_human = False\n",
        "    if (\n",
        "        response.tool_calls\n",
        "        and response.tool_calls[0][\"name\"] == RequestAssistance.__name__\n",
        "    ):\n",
        "        ask_human = True\n",
        "    return {\"messages\": [response], \"ask_human\": ask_human}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f4464d2-288b-4689-aaf0-329a55dcb85c",
      "metadata": {
        "id": "3f4464d2-288b-4689-aaf0-329a55dcb85c"
      },
      "outputs": [],
      "source": [
        "graph_builder = StateGraph(State)\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.add_node(\"tools\", ToolNode(tools=[tool]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f7a0ff3-b671-45c8-8157-ce5db411d370",
      "metadata": {
        "id": "7f7a0ff3-b671-45c8-8157-ce5db411d370"
      },
      "source": [
        "Next, create the \"human\" `node`. This `node` function is mostly a placeholder in our graph that will trigger an interrupt. If the human does __not__ manually update the state during the `interrupt`, it inserts a tool message so the LLM knows the user was requested but didn't respond. This node also unsets the `ask_human` flag so the graph knows not to revisit the node unless further requests are made."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d70b5a4-ce50-47dc-aa43-ffb5c48c46fc",
      "metadata": {
        "id": "1d70b5a4-ce50-47dc-aa43-ffb5c48c46fc"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "\n",
        "\n",
        "def create_response(response: str, ai_message: AIMessage):\n",
        "    return ToolMessage(\n",
        "        content=response,\n",
        "        tool_call_id=ai_message.tool_calls[0][\"id\"],\n",
        "    )\n",
        "\n",
        "\n",
        "def human_node(state: State):\n",
        "    new_messages = []\n",
        "    if not isinstance(state[\"messages\"][-1], ToolMessage):\n",
        "        # Typically, the user will have updated the state during the interrupt.\n",
        "        # If they choose not to, we will include a placeholder ToolMessage to\n",
        "        # let the LLM continue.\n",
        "        new_messages.append(\n",
        "            create_response(\"No response from human.\", state[\"messages\"][-1])\n",
        "        )\n",
        "    return {\n",
        "        # Append the new messages\n",
        "        \"messages\": new_messages,\n",
        "        # Unset the flag\n",
        "        \"ask_human\": False,\n",
        "    }\n",
        "\n",
        "\n",
        "graph_builder.add_node(\"human\", human_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d56e5c65-f7b7-48bd-b0b5-fc8e590eca7d",
      "metadata": {
        "id": "d56e5c65-f7b7-48bd-b0b5-fc8e590eca7d"
      },
      "source": [
        "### Define Edges then Compile\n",
        "\n",
        "Next, define the conditional logic. The `select_next_node` will route to the `human` node if the flag is set. Otherwise, it lets the prebuilt `tools_condition` function choose the next node.\n",
        "\n",
        "Recall that the `tools_condition` function simply checks to see if the `chatbot` has responded with any `tool_calls` in its response message. If so, it routes to the `action` node. Otherwise, it ends the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "586a0d07-8303-47f4-b3cf-3bdd043e762b",
      "metadata": {
        "id": "586a0d07-8303-47f4-b3cf-3bdd043e762b"
      },
      "outputs": [],
      "source": [
        "def select_next_node(state: State):\n",
        "    if state[\"ask_human\"]:\n",
        "        return \"human\"\n",
        "    # Otherwise, we can route as before\n",
        "    return tools_condition(state)\n",
        "\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    select_next_node,\n",
        "    {\"human\": \"human\", \"tools\": \"tools\", \"__end__\": \"__end__\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66cd0bb1-b13e-477e-a08a-a7e657e2c19e",
      "metadata": {
        "id": "66cd0bb1-b13e-477e-a08a-a7e657e2c19e"
      },
      "source": [
        "Finally, add the simple directed edges and compile the graph. These edges instruct the graph to **always** flow from node `a`->`b` whenever `a` finishes executing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84101737-0048-4635-9f68-45b0c508b6b6",
      "metadata": {
        "id": "84101737-0048-4635-9f68-45b0c508b6b6"
      },
      "outputs": [],
      "source": [
        "# The rest is the same\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(\"human\", \"chatbot\")\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
        "graph = graph_builder.compile(\n",
        "    checkpointer=memory,\n",
        "    # We interrupt before 'human' here instead.\n",
        "    interrupt_before=[\"human\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f855593-8690-4a18-9ef8-7f3ccdc335bf",
      "metadata": {
        "id": "7f855593-8690-4a18-9ef8-7f3ccdc335bf"
      },
      "source": [
        "### Visualize\n",
        "If you have the visualization dependencies installed, you can see the graph structure below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3220ae2-cba0-4447-96d1-eb0be4684e59",
      "metadata": {
        "id": "b3220ae2-cba0-4447-96d1-eb0be4684e59"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3b73851-810e-4466-89d8-37fba87e8494",
      "metadata": {
        "id": "a3b73851-810e-4466-89d8-37fba87e8494"
      },
      "source": [
        "The chat bot can either request help from a human (chatbot->select->human), invoke the search engine tool (chatbot->select->action), or directly respond (chatbot->select->__end__). Once an action or request has been made, the graph will transition back to the `chatbot` node to continue operations.\n",
        "\n",
        "### Run Graph\n",
        "Let's see this graph in action. We will request for expert assistance to illustrate our graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1955d79-a1e4-47d0-ba79-b45bd5752a23",
      "metadata": {
        "id": "c1955d79-a1e4-47d0-ba79-b45bd5752a23"
      },
      "outputs": [],
      "source": [
        "user_input = \"I need some expert guidance for building this AI agent. Could you request assistance for me?\"\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3945ea4-8dbd-4e14-ae2a-34da7f05a0c1",
      "metadata": {
        "id": "b3945ea4-8dbd-4e14-ae2a-34da7f05a0c1"
      },
      "source": [
        "### Inspect\n",
        "\n",
        "**Notice:** the LLM has invoked the \"`RequestAssistance`\" tool we provided it, and the interrupt has been set. Let's inspect the graph state to confirm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5320ba05-5696-4194-8278-5385c571264d",
      "metadata": {
        "id": "5320ba05-5696-4194-8278-5385c571264d"
      },
      "outputs": [],
      "source": [
        "snapshot = graph.get_state(config)\n",
        "snapshot.next"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed2dd02e-f0a6-4f63-a7d6-e49ecf40db21",
      "metadata": {
        "id": "ed2dd02e-f0a6-4f63-a7d6-e49ecf40db21"
      },
      "source": [
        "The graph state is indeed **interrupted** before the `'human'` node. We can act as the \"expert\" in this scenario and manually update the state by adding a new ToolMessage with our input.\n",
        "\n",
        "Next, respond to the chatbot's request by:\n",
        "1. Creating a `ToolMessage` with our response. This will be passed back to the `chatbot`.\n",
        "2. Calling `update_state` to manually update the graph state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cbac924-61ce-4282-9b1c-77f9090ea1f5",
      "metadata": {
        "id": "2cbac924-61ce-4282-9b1c-77f9090ea1f5"
      },
      "outputs": [],
      "source": [
        "ai_message = snapshot.values[\"messages\"][-1]\n",
        "human_response = (\n",
        "    \"We, the experts are here to help! We'd recommend you check out LangGraph to build your agent.\"\n",
        "    \" It's much more reliable and extensible than simple autonomous agents.\"\n",
        ")\n",
        "tool_message = create_response(human_response, ai_message)\n",
        "graph.update_state(config, {\"messages\": [tool_message]})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79492363-7fc6-4ec7-977d-9030648029bc",
      "metadata": {
        "id": "79492363-7fc6-4ec7-977d-9030648029bc"
      },
      "source": [
        "You can inspect the state to confirm our response was added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b986c66-1c65-4da8-a404-db7e28f8364e",
      "metadata": {
        "id": "4b986c66-1c65-4da8-a404-db7e28f8364e"
      },
      "outputs": [],
      "source": [
        "graph.get_state(config).values[\"messages\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea6b8616-de10-44d6-a8f0-3ac73c3c3680",
      "metadata": {
        "id": "ea6b8616-de10-44d6-a8f0-3ac73c3c3680"
      },
      "source": [
        "### Resume Graph\n",
        "Next, **resume** the graph by invoking it with `None` as the inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b32914d-4d60-491f-8e11-1e6867e38ffd",
      "metadata": {
        "id": "6b32914d-4d60-491f-8e11-1e6867e38ffd"
      },
      "outputs": [],
      "source": [
        "events = graph.stream(None, config, stream_mode=\"values\")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e0559b-d653-4dab-8928-b001004d14cb",
      "metadata": {
        "id": "48e0559b-d653-4dab-8928-b001004d14cb"
      },
      "source": [
        "### Inspect\n",
        "\n",
        "**Notice** that the chat bot has incorporated the updated state in its final response. Since **everything** was checkpointed, the \"expert\" human in the loop could perform the update at any time without impacting the graph's execution.\n",
        "\n",
        "**Congratulations!** you've now added an additional node to your assistant graph to let the chat bot decide for itself whether or not it needs to interrupt execution. You did so by updating the graph `State` with a new `ask_human` field and modifying the interruption logic when compiling the graph. This lets you dynamically include a human in the loop while maintaining full **memory** every time you execute the graph.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Snippet"
      ],
      "metadata": {
        "id": "l8NFV7xnMV46"
      },
      "id": "l8NFV7xnMV46"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6516baf8-bbb6-4400-b867-0add1a087342",
      "metadata": {
        "id": "6516baf8-bbb6-4400-b867-0add1a087342"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, Union\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    # This flag is new\n",
        "    ask_human: bool\n",
        "\n",
        "\n",
        "class RequestAssistance(BaseModel):\n",
        "    \"\"\"Escalate the conversation to an expert. Use this if you are unable to assist directly or if the user requires support beyond your permissions.\n",
        "\n",
        "    To use this function, relay the user's 'request' so the expert can provide the right guidance.\n",
        "    \"\"\"\n",
        "\n",
        "    request: str\n",
        "\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
        "# We can bind the llm to a tool definition, a pydantic model, or a json schema\n",
        "llm_with_tools = llm.bind_tools(tools + [RequestAssistance])\n",
        "\n",
        "\n",
        "def chatbot(state: State):\n",
        "    response = llm_with_tools.invoke(state[\"messages\"])\n",
        "    ask_human = False\n",
        "    if (\n",
        "        response.tool_calls\n",
        "        and response.tool_calls[0][\"name\"] == RequestAssistance.__name__\n",
        "    ):\n",
        "        ask_human = True\n",
        "    return {\"messages\": [response], \"ask_human\": ask_human}\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.add_node(\"tools\", ToolNode(tools=[tool]))\n",
        "\n",
        "\n",
        "def create_response(response: str, ai_message: AIMessage):\n",
        "    return ToolMessage(\n",
        "        content=response,\n",
        "        tool_call_id=ai_message.tool_calls[0][\"id\"],\n",
        "    )\n",
        "\n",
        "\n",
        "def human_node(state: State):\n",
        "    new_messages = []\n",
        "    if not isinstance(state[\"messages\"][-1], ToolMessage):\n",
        "        # Typically, the user will have updated the state during the interrupt.\n",
        "        # If they choose not to, we will include a placeholder ToolMessage to\n",
        "        # let the LLM continue.\n",
        "        new_messages.append(\n",
        "            create_response(\"No response from human.\", state[\"messages\"][-1])\n",
        "        )\n",
        "    return {\n",
        "        # Append the new messages\n",
        "        \"messages\": new_messages,\n",
        "        # Unset the flag\n",
        "        \"ask_human\": False,\n",
        "    }\n",
        "\n",
        "\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "\n",
        "\n",
        "def select_next_node(state: State):\n",
        "    if state[\"ask_human\"]:\n",
        "        return \"human\"\n",
        "    # Otherwise, we can route as before\n",
        "    return tools_condition(state)\n",
        "\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    select_next_node,\n",
        "    {\"human\": \"human\", \"tools\": \"tools\", \"__end__\": \"__end__\"},\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(\"human\", \"chatbot\")\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
        "graph = graph_builder.compile(\n",
        "    checkpointer=memory,\n",
        "    interrupt_before=[\"human\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05283db2-2f26-4800-8eda-78a4468a3d8f",
      "metadata": {
        "id": "05283db2-2f26-4800-8eda-78a4468a3d8f"
      },
      "source": [
        "## Part 7: Time Travel\n",
        "\n",
        "In a typical chat bot workflow, the user interacts with the bot 1 or more times to accomplish a task. In the previous sections, we saw how to add memory and a human-in-the-loop to be able to checkpoint our graph state and manually override the state to control future responses.\n",
        "\n",
        "But what if you want to let your user start from a previous response and \"branch off\" to explore a separate outcome? Or what if you want users to be able to \"rewind\" your assistant's work to fix some mistakes or try a different strategy (common in applications like autonomous software engineers)?\n",
        "\n",
        "You can create both of these experiences and more using LangGraph's built-in \"time travel\" functionality.\n",
        "\n",
        "In this section, you will \"rewind\" your graph by fetching a checkpoint using the graph's `get_state_history` method. You can then resume execution at this previous point in time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Use Previous Graph"
      ],
      "metadata": {
        "id": "71TPKYaoPFUk"
      },
      "id": "71TPKYaoPFUk"
    },
    {
      "cell_type": "markdown",
      "id": "5414c482-215e-4cc0-9eef-4a8722d2f468",
      "metadata": {
        "id": "5414c482-215e-4cc0-9eef-4a8722d2f468"
      },
      "source": [
        "### Run\n",
        "Let's have our graph take a couple steps. Every step will be checkpointed in its state history:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69071b02-c011-4b7f-90b1-8e89e032322d",
      "metadata": {
        "id": "69071b02-c011-4b7f-90b1-8e89e032322d"
      },
      "outputs": [],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "events = graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            (\"user\", \"I'm learning LangGraph. Could you do some research on it for me?\")\n",
        "        ]\n",
        "    },\n",
        "    config,\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acbec099-e5d2-497f-929e-c548d7bcbf77",
      "metadata": {
        "id": "acbec099-e5d2-497f-929e-c548d7bcbf77"
      },
      "outputs": [],
      "source": [
        "events = graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            (\"user\", \"Ya that's helpful. Maybe I'll build an autonomous agent with it!\")\n",
        "        ]\n",
        "    },\n",
        "    config,\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2e48c77-65f3-4075-8030-ebf943a281f1",
      "metadata": {
        "id": "b2e48c77-65f3-4075-8030-ebf943a281f1"
      },
      "source": [
        "### Replay\n",
        "\n",
        "Now that we've had the agent take a couple steps, we can `replay` the full state history to see everything that occurred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c0dbed5-210d-40ad-b002-0bc52ef28fac",
      "metadata": {
        "id": "6c0dbed5-210d-40ad-b002-0bc52ef28fac"
      },
      "outputs": [],
      "source": [
        "to_replay = None\n",
        "for state in graph.get_state_history(config):\n",
        "    print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n",
        "    print(\"-\" * 80)\n",
        "    if len(state.values[\"messages\"]) == 6:\n",
        "        # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.\n",
        "        to_replay = state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b182019e-bae3-4616-ba1b-f845c0ab6636",
      "metadata": {
        "id": "b182019e-bae3-4616-ba1b-f845c0ab6636"
      },
      "source": [
        "### Inspect\n",
        "\n",
        "**Notice** that checkpoints are saved for every step of the graph. This _spans invocations__ so you can rewind across a full thread's history. We've picked out `to_replay` as a state to resume from. This is the state after the `chatbot` node in the second graph invocation above.\n",
        "\n",
        "Resuming from this point should call the **action** node next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de8d5521-8d71-4093-a657-4920c790802f",
      "metadata": {
        "id": "de8d5521-8d71-4093-a657-4920c790802f"
      },
      "outputs": [],
      "source": [
        "print(to_replay.next)\n",
        "print(to_replay.config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e8c61f5-3a4a-4cce-b81b-43fe1dcc971f",
      "metadata": {
        "id": "7e8c61f5-3a4a-4cce-b81b-43fe1dcc971f"
      },
      "source": [
        "**Notice** that the checkpoint's config (`to_replay.config`) contains a `thread_ts` **timestamp**. Providing this `thread_ts` value tells LangGraph's checkpointer to **load** the state from that moment in time. Let's try it below:\n",
        "\n",
        "### Resume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f17be3-eaf6-495e-a846-49436916b4ab",
      "metadata": {
        "id": "85f17be3-eaf6-495e-a846-49436916b4ab"
      },
      "outputs": [],
      "source": [
        "# The `thread_ts` in the `to_replay.config` corresponds to a state we've persisted to our checkpointer.\n",
        "for event in graph.stream(None, to_replay.config, stream_mode=\"values\"):\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2501fed-2591-420d-98e0-4a3836fb99a8",
      "metadata": {
        "id": "c2501fed-2591-420d-98e0-4a3836fb99a8"
      },
      "source": [
        "Notice that the graph resumed execution from the `**action**` node. You can tell this is the case since the first value printed above is the response from our search engine tool.\n",
        "\n",
        "**Congratulations!** You've now used time-travel checkpoint traversal in LangGraph. Being able to rewind and explore alternative paths opens up a world of possibilities for debugging, experimentation, and interactive applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e584d57f-5aad-4507-815f-0b2e4b64b791",
      "metadata": {
        "id": "e584d57f-5aad-4507-815f-0b2e4b64b791"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Congrats! You've completed the intro tutorial and built a chat bot in LangGraph that supports tool calling, persistent memory, human-in-the-loop interactivity, and even time-travel!\n",
        "\n",
        "The [LangGraph documentation](https://langchain-ai.github.io/langgraph/) is a great resource for diving deeper into the library's capabilities."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "name": "Introduction-to-langgraph.ipynb",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}